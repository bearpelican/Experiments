{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we compare the different validation techniques on an imagenet model already trained to 93%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os, shutil, time, warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import Sampler\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os.path\n",
    "import urllib.request\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "data = Path.home()/'data/imagenet'\n",
    "workers = 7\n",
    "valdir = os.path.join(data, 'validation')\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Image to Aspect ratio mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: sort images by aspect ratio\n",
    "def sort_ar(valdir):\n",
    "    idx2ar_file = data/'sorted_idxar.p'\n",
    "    if os.path.isfile(idx2ar_file): return pickle.load(open(idx2ar_file, 'rb'))\n",
    "    print('Creating AR indexes. Please be patient this may take a couple minutes...')\n",
    "    val_dataset = datasets.ImageFolder(valdir)\n",
    "    sizes = [img[0].size for img in tqdm(val_dataset, total=len(val_dataset))]\n",
    "    idx_ar = [(i, round(s[0]/s[1], 5)) for i,s in enumerate(sizes)]\n",
    "    sorted_idxar = sorted(idx_ar, key=lambda x: x[1])\n",
    "    pickle.dump(sorted_idxar, open(idx2ar_file, 'wb'))\n",
    "    return sorted_idxar\n",
    "\n",
    "# Step 2: chunk images by batch size. This way we can crop each image to the batch aspect ratio mean \n",
    "def chunks(l, n):\n",
    "    n = max(1, n)\n",
    "    return (l[i:i+n] for i in range(0, len(l), n))\n",
    "\n",
    "# Step 3: map image index to batch aspect ratio mean so our transform function knows where to crop\n",
    "def map_idx2ar(idx_ar_sorted, batch_size):\n",
    "    ar_chunks = list(chunks(idx_ar_sorted, batch_size))\n",
    "    idx2ar = {}\n",
    "    ar_means = []\n",
    "    for chunk in ar_chunks:\n",
    "        idxs, ars = list(zip(*chunk))\n",
    "        mean = round(np.mean(ars), 5)\n",
    "        ar_means.append(mean)\n",
    "        for idx in idxs:\n",
    "            idx2ar[idx] = mean\n",
    "    return idx2ar, ar_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_ar_sorted = sort_ar(valdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OR just download it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2ar_path = data/'sorted_idxar.p'\n",
    "url = 'https://s3-us-west-2.amazonaws.com/ashaw-fastai-imagenet/sorted_idxar.p'\n",
    "if not os.path.exists(idx2ar_path): urllib.request.urlretrieve(url, idx2ar_path)\n",
    "idx_ar_sorted = sort_ar(valdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValDataset(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None, target_transform=None):\n",
    "        super().__init__(root, transform, target_transform)\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.imgs[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            for tfm in self.transform:\n",
    "                if isinstance(tfm, RectangularCropTfm): sample = tfm(sample, index)\n",
    "                else: sample = tfm(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return sample, target\n",
    "\n",
    "    \n",
    "# Essentially a sequential sampler\n",
    "class SequentialIndexSampler(Sampler):\n",
    "    def __init__(self, indices): self.indices = indices\n",
    "    def __len__(self): return len(self.indices)\n",
    "    def __iter__(self): return iter(self.indices)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RectangularCropTfm(object):\n",
    "    def __init__(self, idx2ar, target_size):\n",
    "        self.idx2ar, self.target_size = idx2ar, target_size\n",
    "    def __call__(self, img, idx):\n",
    "        target_ar = self.idx2ar[idx]\n",
    "        if target_ar < 1: \n",
    "            w = int(self.target_size/target_ar)\n",
    "            size = (w//8*8, self.target_size)\n",
    "        else: \n",
    "            h = int(self.target_size*target_ar)\n",
    "            size = (self.target_size, h//8*8)\n",
    "        return torchvision.transforms.functional.center_crop(img, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Function with TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, aug_loader=None, num_augmentations=0):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "\n",
    "    val_iter = iter(val_loader)\n",
    "    aug_iters = [iter(aug_loader) for i in range(num_augmentations)]\n",
    "    prec5_arr = []\n",
    "    for i in range(len(val_loader)):\n",
    "        def get_output(dl_iter):\n",
    "            input,target = next(dl_iter)\n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            with torch.no_grad():\n",
    "                output = model(Variable(input))\n",
    "                loss = criterion(output, Variable(target))\n",
    "            return output, loss, input, target\n",
    "        \n",
    "        output,loss,input,target = get_output(val_iter)\n",
    "        for aug_iter in aug_iters:\n",
    "            o,l,_,_ = get_output(aug_iter)\n",
    "            output.add_(o)\n",
    "            loss.add_(l)\n",
    "        loss.div_(num_augmentations+1)\n",
    "        \n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        reduced_loss = loss.data\n",
    "            \n",
    "        losses.update(to_python_float(reduced_loss), input.size(0))\n",
    "        top1.update(to_python_float(prec1), input.size(0))\n",
    "        top5.update(to_python_float(prec5), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if (i != 0) and (i%50 == 0):\n",
    "            prec5_arr.append(top5.val)\n",
    "            output = ('Test: [{0}/{1}]\\t' \\\n",
    "                    + 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t' \\\n",
    "                    + 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t' \\\n",
    "                    + 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t' \\\n",
    "                    + 'Prec@5 {top5.val:.3f} ({top5.avg:.3f})').format(\n",
    "                    i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                    top1=top1, top5=top5)\n",
    "            print(output)\n",
    "\n",
    "    time_diff = datetime.now()-start_time\n",
    "    print(f'~~{float(time_diff.total_seconds() / 3600.0)}\\t{top5.avg:.3f}\\n')\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n",
    "\n",
    "    return prec5_arr\n",
    "\n",
    "# item() is a recent addition, so this helps with backward compatibility.\n",
    "def to_python_float(t):\n",
    "    if hasattr(t, 'item'): return t.item()\n",
    "    else: return t[0]\n",
    "    \n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self): self.reset()\n",
    "    def reset(self): self.val = self.avg = self.sum = self.count = 0\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resnet\n",
    "model = resnet.resnet50()\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = data/'93_base_resnet50.p'\n",
    "url = 'https://s3-us-west-2.amazonaws.com/ashaw-fastai-imagenet/93_base_resnet50.p'\n",
    "if not os.path.exists(model_path): urllib.request.urlretrieve(url, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfm_wrapper_idx(fn):\n",
    "    return lambda x,idx: (fn(x),idx)\n",
    "def tfm_wrapper(fn):\n",
    "    return lambda x,idx: fn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValDataset(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None, target_transform=None):\n",
    "        super().__init__(root, transform, target_transform)\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.imgs[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            for tfm in self.transform:\n",
    "                if isinstance(tfm, RectangularCropTfm): sample = tfm(sample, index)\n",
    "                else: sample = tfm(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return sample, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global dataset settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_bs = 128\n",
    "target_size = 288\n",
    "\n",
    "idx_sorted, _ = zip(*idx_ar_sorted)\n",
    "idx2ar, ar_means = map_idx2ar(idx_ar_sorted, val_bs)\n",
    "val_sampler_ar = SequentialIndexSampler(idx_sorted)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "tensor_tfm = [transforms.ToTensor(), normalize]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test validation with aspect ratio transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_dataset_ar = ValDataset(valdir, [RectangularCropTfm(idx2ar, target_size)] + tensor_tfm)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset_ar, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "ar_prec5 = validate(val_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Original Validation Technique ("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [10/391]\tTime 0.099 (0.357)\tLoss 1.2266 (1.0176)\tPrec@1 74.219 (73.509)\tPrec@5 88.281 (91.832)\n",
      "Test: [20/391]\tTime 0.099 (0.286)\tLoss 0.8472 (1.0271)\tPrec@1 77.344 (73.772)\tPrec@5 95.312 (91.592)\n",
      "Test: [30/391]\tTime 0.099 (0.260)\tLoss 0.9014 (1.0098)\tPrec@1 75.781 (74.068)\tPrec@5 94.531 (91.986)\n",
      "Test: [40/391]\tTime 0.099 (0.259)\tLoss 1.1865 (0.9693)\tPrec@1 69.531 (75.191)\tPrec@5 89.062 (92.454)\n",
      "Test: [50/391]\tTime 0.099 (0.255)\tLoss 1.2139 (1.0197)\tPrec@1 70.312 (74.265)\tPrec@5 91.406 (91.850)\n",
      "Test: [60/391]\tTime 0.668 (0.249)\tLoss 1.4102 (1.0635)\tPrec@1 62.500 (73.309)\tPrec@5 88.281 (91.317)\n",
      "Test: [70/391]\tTime 0.099 (0.238)\tLoss 0.9673 (1.0534)\tPrec@1 78.125 (73.537)\tPrec@5 92.188 (91.395)\n",
      "Test: [80/391]\tTime 0.099 (0.230)\tLoss 0.9507 (1.0258)\tPrec@1 75.781 (74.199)\tPrec@5 90.625 (91.763)\n",
      "Test: [90/391]\tTime 0.099 (0.229)\tLoss 0.8936 (1.0128)\tPrec@1 76.562 (74.493)\tPrec@5 95.312 (92.033)\n",
      "Test: [100/391]\tTime 0.099 (0.224)\tLoss 0.8130 (0.9908)\tPrec@1 78.906 (74.961)\tPrec@5 94.531 (92.358)\n",
      "Test: [110/391]\tTime 0.100 (0.225)\tLoss 1.2002 (0.9830)\tPrec@1 71.875 (75.183)\tPrec@5 90.625 (92.476)\n",
      "Test: [120/391]\tTime 0.099 (0.223)\tLoss 1.2520 (0.9991)\tPrec@1 75.000 (74.858)\tPrec@5 88.281 (92.246)\n",
      "Test: [130/391]\tTime 0.190 (0.222)\tLoss 0.8667 (0.9871)\tPrec@1 79.688 (75.072)\tPrec@5 89.844 (92.343)\n",
      "Test: [140/391]\tTime 0.419 (0.224)\tLoss 0.7607 (0.9731)\tPrec@1 80.469 (75.360)\tPrec@5 95.312 (92.537)\n",
      "Test: [150/391]\tTime 0.099 (0.222)\tLoss 0.5669 (0.9639)\tPrec@1 82.812 (75.579)\tPrec@5 96.094 (92.627)\n",
      "Test: [160/391]\tTime 0.123 (0.222)\tLoss 0.8950 (0.9493)\tPrec@1 77.344 (76.009)\tPrec@5 89.844 (92.770)\n",
      "Test: [170/391]\tTime 1.115 (0.225)\tLoss 0.7007 (0.9423)\tPrec@1 81.250 (76.183)\tPrec@5 93.750 (92.827)\n",
      "Test: [180/391]\tTime 0.275 (0.222)\tLoss 0.8818 (0.9371)\tPrec@1 77.344 (76.239)\tPrec@5 94.531 (92.878)\n",
      "Test: [190/391]\tTime 0.099 (0.220)\tLoss 0.8525 (0.9317)\tPrec@1 77.344 (76.333)\tPrec@5 94.531 (92.981)\n",
      "Test: [200/391]\tTime 0.115 (0.220)\tLoss 0.7744 (0.9165)\tPrec@1 77.344 (76.679)\tPrec@5 98.438 (93.144)\n",
      "Test: [210/391]\tTime 0.115 (0.221)\tLoss 0.7441 (0.9105)\tPrec@1 77.344 (76.792)\tPrec@5 93.750 (93.269)\n",
      "Test: [220/391]\tTime 0.103 (0.219)\tLoss 0.3679 (0.9034)\tPrec@1 89.844 (76.821)\tPrec@5 96.875 (93.411)\n",
      "Test: [230/391]\tTime 0.099 (0.221)\tLoss 1.1699 (0.8989)\tPrec@1 71.094 (76.945)\tPrec@5 87.500 (93.442)\n",
      "Test: [240/391]\tTime 0.099 (0.221)\tLoss 1.6162 (0.9109)\tPrec@1 65.625 (76.689)\tPrec@5 85.156 (93.306)\n",
      "Test: [250/391]\tTime 0.536 (0.223)\tLoss 0.7603 (0.9162)\tPrec@1 78.125 (76.572)\tPrec@5 94.531 (93.252)\n",
      "Test: [260/391]\tTime 0.098 (0.221)\tLoss 1.1455 (0.9260)\tPrec@1 78.906 (76.458)\tPrec@5 86.719 (93.094)\n",
      "Test: [270/391]\tTime 0.987 (0.224)\tLoss 0.7974 (0.9306)\tPrec@1 82.031 (76.343)\tPrec@5 94.531 (93.075)\n",
      "Test: [280/391]\tTime 0.099 (0.225)\tLoss 1.3018 (0.9394)\tPrec@1 72.656 (76.173)\tPrec@5 87.500 (92.910)\n",
      "Test: [290/391]\tTime 0.099 (0.223)\tLoss 1.3789 (0.9482)\tPrec@1 66.406 (76.009)\tPrec@5 84.375 (92.794)\n",
      "Test: [300/391]\tTime 0.099 (0.224)\tLoss 0.8672 (0.9509)\tPrec@1 71.875 (75.885)\tPrec@5 97.656 (92.787)\n",
      "Test: [310/391]\tTime 0.110 (0.222)\tLoss 1.2500 (0.9512)\tPrec@1 74.219 (75.831)\tPrec@5 89.844 (92.798)\n",
      "Test: [320/391]\tTime 0.099 (0.225)\tLoss 1.0107 (0.9484)\tPrec@1 71.094 (75.857)\tPrec@5 93.750 (92.840)\n",
      "Test: [330/391]\tTime 0.100 (0.223)\tLoss 0.8877 (0.9450)\tPrec@1 77.344 (75.949)\tPrec@5 96.094 (92.886)\n",
      "Test: [340/391]\tTime 0.613 (0.223)\tLoss 0.6675 (0.9438)\tPrec@1 86.719 (75.990)\tPrec@5 96.875 (92.909)\n",
      "Test: [350/391]\tTime 0.129 (0.222)\tLoss 0.9844 (0.9407)\tPrec@1 75.000 (76.037)\tPrec@5 92.188 (92.946)\n",
      "Test: [360/391]\tTime 0.099 (0.220)\tLoss 0.6685 (0.9343)\tPrec@1 82.812 (76.162)\tPrec@5 97.656 (93.029)\n",
      "Test: [370/391]\tTime 0.099 (0.221)\tLoss 1.3906 (0.9357)\tPrec@1 65.625 (76.122)\tPrec@5 85.156 (93.019)\n",
      "Test: [380/391]\tTime 0.548 (0.221)\tLoss 0.6187 (0.9328)\tPrec@1 82.031 (76.193)\tPrec@5 96.094 (93.077)\n",
      "Test: [390/391]\tTime 1.558 (0.224)\tLoss 1.2861 (0.9355)\tPrec@1 71.250 (76.144)\tPrec@5 86.250 (93.046)\n",
      "~~0.024292703333333335\t93.046\n",
      "\n",
      " * Prec@1 76.144 Prec@5 93.046\n"
     ]
    }
   ],
   "source": [
    "val_tfms = [transforms.Resize(int(target_size*1.14)), transforms.CenterCrop(target_size)] + tensor_tfm\n",
    "val_dataset = datasets.ImageFolder(valdir,  transforms.Compose(val_tfms))\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "orig_prec5 = validate(val_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test AR with size*1.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [10/391]\tTime 0.146 (1.121)\tLoss 1.0566 (0.9035)\tPrec@1 76.562 (76.207)\tPrec@5 92.188 (93.821)\n",
      "Test: [20/391]\tTime 2.574 (0.854)\tLoss 0.8447 (0.9367)\tPrec@1 80.469 (76.153)\tPrec@5 95.312 (93.787)\n",
      "Test: [30/391]\tTime 0.127 (0.881)\tLoss 0.9351 (0.9399)\tPrec@1 74.219 (76.008)\tPrec@5 92.969 (93.674)\n",
      "Test: [40/391]\tTime 0.127 (0.697)\tLoss 1.1084 (0.9104)\tPrec@1 71.094 (76.963)\tPrec@5 92.188 (93.921)\n",
      "Test: [50/391]\tTime 0.159 (0.604)\tLoss 1.0420 (0.9545)\tPrec@1 75.000 (76.011)\tPrec@5 91.406 (93.290)\n",
      "Test: [60/391]\tTime 0.568 (0.544)\tLoss 1.3164 (0.9920)\tPrec@1 65.625 (75.102)\tPrec@5 89.844 (92.777)\n",
      "Test: [70/391]\tTime 0.158 (0.519)\tLoss 0.9653 (0.9864)\tPrec@1 77.344 (75.165)\tPrec@5 92.188 (92.749)\n",
      "Test: [80/391]\tTime 2.509 (0.546)\tLoss 0.8687 (0.9627)\tPrec@1 76.562 (75.714)\tPrec@5 92.969 (93.036)\n",
      "Test: [90/391]\tTime 0.112 (0.555)\tLoss 0.8076 (0.9521)\tPrec@1 81.250 (75.893)\tPrec@5 96.875 (93.192)\n",
      "Test: [100/391]\tTime 0.098 (0.584)\tLoss 0.8130 (0.9349)\tPrec@1 78.906 (76.214)\tPrec@5 94.531 (93.410)\n",
      "Test: [110/391]\tTime 0.099 (0.540)\tLoss 1.2002 (0.9321)\tPrec@1 71.875 (76.323)\tPrec@5 90.625 (93.433)\n",
      "Test: [120/391]\tTime 1.003 (0.517)\tLoss 1.2520 (0.9524)\tPrec@1 75.000 (75.904)\tPrec@5 88.281 (93.124)\n",
      "Test: [130/391]\tTime 2.071 (0.532)\tLoss 0.8926 (0.9445)\tPrec@1 74.219 (75.966)\tPrec@5 91.406 (93.154)\n",
      "Test: [140/391]\tTime 1.590 (0.541)\tLoss 0.7861 (0.9332)\tPrec@1 78.906 (76.141)\tPrec@5 93.750 (93.296)\n",
      "Test: [150/391]\tTime 0.135 (0.538)\tLoss 0.5415 (0.9255)\tPrec@1 85.156 (76.340)\tPrec@5 97.656 (93.367)\n",
      "Test: [160/391]\tTime 0.122 (0.529)\tLoss 0.8467 (0.9129)\tPrec@1 78.125 (76.723)\tPrec@5 89.844 (93.478)\n",
      "Test: [170/391]\tTime 0.133 (0.515)\tLoss 0.6934 (0.9075)\tPrec@1 82.812 (76.887)\tPrec@5 95.312 (93.508)\n",
      "Test: [180/391]\tTime 0.125 (0.505)\tLoss 0.8730 (0.9035)\tPrec@1 75.000 (76.925)\tPrec@5 96.875 (93.534)\n",
      "Test: [190/391]\tTime 0.125 (0.487)\tLoss 0.9033 (0.8997)\tPrec@1 77.344 (76.976)\tPrec@5 92.969 (93.603)\n",
      "Test: [200/391]\tTime 0.125 (0.474)\tLoss 0.7612 (0.8876)\tPrec@1 80.469 (77.289)\tPrec@5 94.531 (93.731)\n",
      "Test: [210/391]\tTime 0.258 (0.462)\tLoss 0.8315 (0.8845)\tPrec@1 76.562 (77.362)\tPrec@5 93.750 (93.802)\n",
      "Test: [220/391]\tTime 0.141 (0.453)\tLoss 0.3738 (0.8795)\tPrec@1 90.625 (77.414)\tPrec@5 98.438 (93.909)\n",
      "Test: [230/391]\tTime 0.657 (0.449)\tLoss 1.1123 (0.8763)\tPrec@1 75.781 (77.509)\tPrec@5 88.281 (93.929)\n",
      "Test: [240/391]\tTime 0.158 (0.437)\tLoss 1.5410 (0.8861)\tPrec@1 67.188 (77.289)\tPrec@5 86.719 (93.854)\n",
      "Test: [250/391]\tTime 0.130 (0.429)\tLoss 0.6997 (0.8902)\tPrec@1 80.469 (77.169)\tPrec@5 96.094 (93.815)\n",
      "Test: [260/391]\tTime 0.126 (0.421)\tLoss 1.0957 (0.8986)\tPrec@1 76.562 (77.077)\tPrec@5 89.844 (93.672)\n",
      "Test: [270/391]\tTime 0.451 (0.413)\tLoss 0.7739 (0.9022)\tPrec@1 82.812 (76.975)\tPrec@5 94.531 (93.649)\n",
      "Test: [280/391]\tTime 0.170 (0.407)\tLoss 1.1123 (0.9097)\tPrec@1 78.906 (76.835)\tPrec@5 85.938 (93.505)\n",
      "Test: [290/391]\tTime 0.127 (0.399)\tLoss 1.2822 (0.9185)\tPrec@1 66.406 (76.627)\tPrec@5 89.844 (93.380)\n",
      "Test: [300/391]\tTime 0.498 (0.394)\tLoss 0.8950 (0.9209)\tPrec@1 77.344 (76.542)\tPrec@5 96.875 (93.355)\n",
      "Test: [310/391]\tTime 0.125 (0.388)\tLoss 1.2070 (0.9210)\tPrec@1 74.219 (76.500)\tPrec@5 90.625 (93.366)\n",
      "Test: [320/391]\tTime 0.151 (0.394)\tLoss 1.0234 (0.9189)\tPrec@1 71.875 (76.516)\tPrec@5 91.406 (93.380)\n",
      "Test: [330/391]\tTime 0.159 (0.398)\tLoss 0.8271 (0.9162)\tPrec@1 79.688 (76.607)\tPrec@5 95.312 (93.434)\n",
      "Test: [340/391]\tTime 0.143 (0.404)\tLoss 0.6929 (0.9148)\tPrec@1 86.719 (76.656)\tPrec@5 96.094 (93.452)\n",
      "Test: [350/391]\tTime 0.145 (0.396)\tLoss 0.9077 (0.9122)\tPrec@1 75.781 (76.738)\tPrec@5 91.406 (93.492)\n",
      "Test: [360/391]\tTime 0.145 (0.397)\tLoss 0.7231 (0.9079)\tPrec@1 81.250 (76.848)\tPrec@5 97.656 (93.562)\n",
      "Test: [370/391]\tTime 0.145 (0.390)\tLoss 1.2930 (0.9090)\tPrec@1 71.875 (76.847)\tPrec@5 88.281 (93.573)\n",
      "Test: [380/391]\tTime 0.145 (0.387)\tLoss 0.6572 (0.9065)\tPrec@1 82.031 (76.921)\tPrec@5 96.094 (93.625)\n",
      "Test: [390/391]\tTime 3.846 (0.440)\tLoss 0.9429 (0.9074)\tPrec@1 77.500 (76.884)\tPrec@5 92.500 (93.644)\n",
      "~~0.04782311111111111\t93.644\n",
      "\n",
      " * Prec@1 76.884 Prec@5 93.644\n"
     ]
    }
   ],
   "source": [
    "val_ar_tfms = [transforms.Resize(int(target_size*1.14)), RectangularCropTfm(idx2ar, target_size)]\n",
    "val_dataset_ar_rs = ValDataset(valdir, val_ar_tfms+tensor_tfm)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset_ar_rs, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "ar_rs_prec5 = validate(val_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_batch_means = [np.array(c).mean() for c in chunks(ar_means, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DawnValidation</th>\n",
       "      <th>BatchAspectRatioValidation</th>\n",
       "      <th>AR Mean</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.28125</td>\n",
       "      <td>92.18750</td>\n",
       "      <td>0.636959</td>\n",
       "      <td>3.90625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95.31250</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>0.667781</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94.53125</td>\n",
       "      <td>92.96875</td>\n",
       "      <td>0.717153</td>\n",
       "      <td>-1.56250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.06250</td>\n",
       "      <td>92.18750</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>3.12500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91.40625</td>\n",
       "      <td>91.40625</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>88.28125</td>\n",
       "      <td>89.84375</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.56250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>92.18750</td>\n",
       "      <td>92.18750</td>\n",
       "      <td>0.750127</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>90.62500</td>\n",
       "      <td>92.96875</td>\n",
       "      <td>0.777394</td>\n",
       "      <td>2.34375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>95.31250</td>\n",
       "      <td>96.87500</td>\n",
       "      <td>0.830958</td>\n",
       "      <td>1.56250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>94.53125</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>0.922669</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>90.62500</td>\n",
       "      <td>90.62500</td>\n",
       "      <td>0.995605</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>88.28125</td>\n",
       "      <td>88.28125</td>\n",
       "      <td>1.000317</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>89.84375</td>\n",
       "      <td>91.40625</td>\n",
       "      <td>1.045130</td>\n",
       "      <td>1.56250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>95.31250</td>\n",
       "      <td>93.75000</td>\n",
       "      <td>1.125204</td>\n",
       "      <td>-1.56250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>96.09375</td>\n",
       "      <td>97.65625</td>\n",
       "      <td>1.197685</td>\n",
       "      <td>1.56250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>89.84375</td>\n",
       "      <td>89.84375</td>\n",
       "      <td>1.245955</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>93.75000</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>1.275184</td>\n",
       "      <td>1.56250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>94.53125</td>\n",
       "      <td>96.87500</td>\n",
       "      <td>1.319574</td>\n",
       "      <td>2.34375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>94.53125</td>\n",
       "      <td>92.96875</td>\n",
       "      <td>1.333231</td>\n",
       "      <td>-1.56250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>98.43750</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>-3.90625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>93.75000</td>\n",
       "      <td>93.75000</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>96.87500</td>\n",
       "      <td>98.43750</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>1.56250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>87.50000</td>\n",
       "      <td>88.28125</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>0.78125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>85.15625</td>\n",
       "      <td>86.71875</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>1.56250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>94.53125</td>\n",
       "      <td>96.09375</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>1.56250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>86.71875</td>\n",
       "      <td>89.84375</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>3.12500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>94.53125</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>87.50000</td>\n",
       "      <td>85.93750</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>-1.56250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>84.37500</td>\n",
       "      <td>89.84375</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>5.46875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>97.65625</td>\n",
       "      <td>96.87500</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>-0.78125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>89.84375</td>\n",
       "      <td>90.62500</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>0.78125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>93.75000</td>\n",
       "      <td>91.40625</td>\n",
       "      <td>1.341604</td>\n",
       "      <td>-2.34375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>96.09375</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>1.398043</td>\n",
       "      <td>-0.78125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>96.87500</td>\n",
       "      <td>96.09375</td>\n",
       "      <td>1.465487</td>\n",
       "      <td>-0.78125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>92.18750</td>\n",
       "      <td>91.40625</td>\n",
       "      <td>1.495881</td>\n",
       "      <td>-0.78125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>97.65625</td>\n",
       "      <td>97.65625</td>\n",
       "      <td>1.500512</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>85.15625</td>\n",
       "      <td>88.28125</td>\n",
       "      <td>1.501500</td>\n",
       "      <td>3.12500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>96.09375</td>\n",
       "      <td>96.09375</td>\n",
       "      <td>1.503053</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>86.25000</td>\n",
       "      <td>92.50000</td>\n",
       "      <td>1.675084</td>\n",
       "      <td>6.25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    DawnValidation  BatchAspectRatioValidation   AR Mean  Difference\n",
       "0         88.28125                    92.18750  0.636959     3.90625\n",
       "1         95.31250                    95.31250  0.667781     0.00000\n",
       "2         94.53125                    92.96875  0.717153    -1.56250\n",
       "3         89.06250                    92.18750  0.750000     3.12500\n",
       "4         91.40625                    91.40625  0.750000     0.00000\n",
       "5         88.28125                    89.84375  0.750000     1.56250\n",
       "6         92.18750                    92.18750  0.750127     0.00000\n",
       "7         90.62500                    92.96875  0.777394     2.34375\n",
       "8         95.31250                    96.87500  0.830958     1.56250\n",
       "9         94.53125                    94.53125  0.922669     0.00000\n",
       "10        90.62500                    90.62500  0.995605     0.00000\n",
       "11        88.28125                    88.28125  1.000317     0.00000\n",
       "12        89.84375                    91.40625  1.045130     1.56250\n",
       "13        95.31250                    93.75000  1.125204    -1.56250\n",
       "14        96.09375                    97.65625  1.197685     1.56250\n",
       "15        89.84375                    89.84375  1.245955     0.00000\n",
       "16        93.75000                    95.31250  1.275184     1.56250\n",
       "17        94.53125                    96.87500  1.319574     2.34375\n",
       "18        94.53125                    92.96875  1.333231    -1.56250\n",
       "19        98.43750                    94.53125  1.333330    -3.90625\n",
       "20        93.75000                    93.75000  1.333330     0.00000\n",
       "21        96.87500                    98.43750  1.333330     1.56250\n",
       "22        87.50000                    88.28125  1.333330     0.78125\n",
       "23        85.15625                    86.71875  1.333330     1.56250\n",
       "24        94.53125                    96.09375  1.333330     1.56250\n",
       "25        86.71875                    89.84375  1.333330     3.12500\n",
       "26        94.53125                    94.53125  1.333330     0.00000\n",
       "27        87.50000                    85.93750  1.333330    -1.56250\n",
       "28        84.37500                    89.84375  1.333330     5.46875\n",
       "29        97.65625                    96.87500  1.333330    -0.78125\n",
       "30        89.84375                    90.62500  1.333330     0.78125\n",
       "31        93.75000                    91.40625  1.341604    -2.34375\n",
       "32        96.09375                    95.31250  1.398043    -0.78125\n",
       "33        96.87500                    96.09375  1.465487    -0.78125\n",
       "34        92.18750                    91.40625  1.495881    -0.78125\n",
       "35        97.65625                    97.65625  1.500512     0.00000\n",
       "36        85.15625                    88.28125  1.501500     3.12500\n",
       "37        96.09375                    96.09375  1.503053     0.00000\n",
       "38        86.25000                    92.50000  1.675084     6.25000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'DawnValidation': orig_prec5, \n",
    "     'BatchAspectRatioValidation': ar_rs_prec5, \n",
    "     'AR Mean': ar_batch_means[:-1],\n",
    "     'Difference': np.array(ar_rs_prec5)-np.array(orig_prec5)}\n",
    "df = pd.DataFrame(data=d);\n",
    "df.to_csv('ar_tests.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test TTA with original validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_scale = 0.5\n",
    "trn_tfms = [\n",
    "        transforms.RandomResizedCrop(target_size, scale=(min_scale, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ] + tensor_tfm\n",
    "aug_dataset = datasets.ImageFolder(valdir, transforms.Compose(trn_tfms))\n",
    "\n",
    "val_tfms = [transforms.Resize(int(target_size*1.14)), transforms.CenterCrop(target_size)] + tensor_tfm\n",
    "val_dataset = datasets.ImageFolder(valdir,  transforms.Compose(val_tfms))\n",
    "\n",
    "aug_loader = torch.utils.data.DataLoader(\n",
    "    aug_dataset, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True, sampler=val_sampler_ar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [10/391]\tTime 0.693 (1.828)\tLoss 1.2256 (1.0097)\tPrec@1 76.562 (75.781)\tPrec@5 91.406 (93.253)\n",
      "Test: [20/391]\tTime 0.813 (1.495)\tLoss 0.9438 (1.0272)\tPrec@1 78.125 (75.521)\tPrec@5 95.312 (93.118)\n",
      "Test: [30/391]\tTime 0.587 (1.356)\tLoss 0.9507 (1.0202)\tPrec@1 74.219 (75.806)\tPrec@5 94.531 (93.296)\n",
      "Test: [40/391]\tTime 0.940 (1.264)\tLoss 1.2256 (0.9870)\tPrec@1 75.781 (76.753)\tPrec@5 91.406 (93.636)\n",
      "Test: [50/391]\tTime 0.612 (1.234)\tLoss 1.2236 (1.0396)\tPrec@1 73.438 (75.689)\tPrec@5 90.625 (93.137)\n",
      "Test: [60/391]\tTime 0.839 (1.205)\tLoss 1.4375 (1.0847)\tPrec@1 64.062 (74.590)\tPrec@5 88.281 (92.636)\n",
      "Test: [70/391]\tTime 0.723 (1.208)\tLoss 1.0352 (1.0779)\tPrec@1 77.344 (74.747)\tPrec@5 92.188 (92.606)\n",
      "Test: [80/391]\tTime 0.640 (1.169)\tLoss 1.0293 (1.0525)\tPrec@1 78.906 (75.347)\tPrec@5 92.969 (92.834)\n",
      "Test: [90/391]\tTime 0.759 (1.167)\tLoss 0.9678 (1.0403)\tPrec@1 75.781 (75.592)\tPrec@5 94.531 (93.012)\n",
      "Test: [100/391]\tTime 0.699 (1.153)\tLoss 0.8857 (1.0190)\tPrec@1 79.688 (76.006)\tPrec@5 94.531 (93.239)\n",
      "Test: [110/391]\tTime 3.163 (1.159)\tLoss 1.2578 (1.0126)\tPrec@1 70.312 (76.154)\tPrec@5 90.625 (93.257)\n",
      "Test: [120/391]\tTime 0.682 (1.146)\tLoss 1.2969 (1.0302)\tPrec@1 77.344 (75.794)\tPrec@5 88.281 (93.001)\n",
      "Test: [130/391]\tTime 0.531 (1.149)\tLoss 0.9209 (1.0185)\tPrec@1 78.906 (75.984)\tPrec@5 94.531 (93.106)\n",
      "Test: [140/391]\tTime 0.564 (1.132)\tLoss 0.7905 (1.0043)\tPrec@1 80.469 (76.213)\tPrec@5 95.312 (93.246)\n",
      "Test: [150/391]\tTime 0.631 (1.131)\tLoss 0.5991 (0.9948)\tPrec@1 83.594 (76.428)\tPrec@5 96.875 (93.326)\n",
      "Test: [160/391]\tTime 0.636 (1.126)\tLoss 0.8936 (0.9796)\tPrec@1 77.344 (76.791)\tPrec@5 90.625 (93.420)\n",
      "Test: [170/391]\tTime 0.808 (1.122)\tLoss 0.7021 (0.9717)\tPrec@1 82.031 (76.933)\tPrec@5 96.094 (93.471)\n",
      "Test: [180/391]\tTime 6.242 (1.146)\tLoss 0.8867 (0.9664)\tPrec@1 79.688 (77.011)\tPrec@5 95.312 (93.504)\n",
      "Test: [190/391]\tTime 0.570 (1.114)\tLoss 0.8799 (0.9609)\tPrec@1 78.125 (77.102)\tPrec@5 95.312 (93.595)\n",
      "Test: [200/391]\tTime 0.612 (1.095)\tLoss 0.8218 (0.9472)\tPrec@1 80.469 (77.421)\tPrec@5 96.875 (93.742)\n",
      "Test: [210/391]\tTime 0.788 (1.098)\tLoss 0.8032 (0.9410)\tPrec@1 77.344 (77.544)\tPrec@5 94.531 (93.828)\n",
      "Test: [220/391]\tTime 0.728 (1.107)\tLoss 0.3774 (0.9340)\tPrec@1 90.625 (77.591)\tPrec@5 98.438 (93.955)\n",
      "Test: [230/391]\tTime 0.824 (1.120)\tLoss 1.1465 (0.9293)\tPrec@1 77.344 (77.736)\tPrec@5 87.500 (93.987)\n",
      "Test: [240/391]\tTime 1.409 (1.118)\tLoss 1.5859 (0.9402)\tPrec@1 67.188 (77.499)\tPrec@5 86.719 (93.883)\n",
      "Test: [250/391]\tTime 2.768 (1.126)\tLoss 0.8008 (0.9451)\tPrec@1 79.688 (77.375)\tPrec@5 96.094 (93.840)\n",
      "Test: [260/391]\tTime 0.944 (1.119)\tLoss 1.1797 (0.9546)\tPrec@1 78.125 (77.251)\tPrec@5 89.062 (93.705)\n",
      "Test: [270/391]\tTime 0.944 (1.115)\tLoss 0.8398 (0.9584)\tPrec@1 79.688 (77.162)\tPrec@5 94.531 (93.698)\n",
      "Test: [280/391]\tTime 0.657 (1.116)\tLoss 1.2998 (0.9666)\tPrec@1 73.438 (76.985)\tPrec@5 88.281 (93.564)\n",
      "Test: [290/391]\tTime 0.780 (1.111)\tLoss 1.3955 (0.9756)\tPrec@1 67.188 (76.801)\tPrec@5 86.719 (93.439)\n",
      "Test: [300/391]\tTime 0.614 (1.112)\tLoss 0.9463 (0.9783)\tPrec@1 73.438 (76.723)\tPrec@5 96.875 (93.418)\n",
      "Test: [310/391]\tTime 1.175 (1.109)\tLoss 1.2871 (0.9783)\tPrec@1 75.000 (76.698)\tPrec@5 89.844 (93.423)\n",
      "Test: [320/391]\tTime 1.422 (1.106)\tLoss 0.9961 (0.9751)\tPrec@1 75.000 (76.723)\tPrec@5 92.969 (93.458)\n",
      "Test: [330/391]\tTime 0.600 (1.103)\tLoss 0.9023 (0.9715)\tPrec@1 76.562 (76.789)\tPrec@5 95.312 (93.507)\n",
      "Test: [340/391]\tTime 0.915 (1.094)\tLoss 0.6689 (0.9698)\tPrec@1 87.500 (76.835)\tPrec@5 96.094 (93.528)\n",
      "Test: [350/391]\tTime 0.597 (1.092)\tLoss 0.9595 (0.9664)\tPrec@1 76.562 (76.892)\tPrec@5 94.531 (93.574)\n",
      "Test: [360/391]\tTime 0.777 (1.088)\tLoss 0.7080 (0.9603)\tPrec@1 82.812 (76.995)\tPrec@5 97.656 (93.653)\n",
      "Test: [370/391]\tTime 0.729 (1.085)\tLoss 1.3594 (0.9613)\tPrec@1 71.094 (77.009)\tPrec@5 87.500 (93.645)\n",
      "Test: [380/391]\tTime 0.489 (1.088)\tLoss 0.6748 (0.9585)\tPrec@1 82.812 (77.079)\tPrec@5 95.312 (93.693)\n",
      "Test: [390/391]\tTime 0.312 (1.072)\tLoss 1.1768 (0.9600)\tPrec@1 73.750 (77.032)\tPrec@5 88.750 (93.682)\n",
      "~~0.11647516277777778\t93.682\n",
      "\n",
      " * Prec@1 77.032 Prec@5 93.682\n"
     ]
    }
   ],
   "source": [
    "tta_prec5 = validate(val_loader, model, criterion, aug_loader=aug_loader, num_augmentations=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test TTA with size*1.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [10/391]\tTime 0.543 (1.979)\tLoss 1.2217 (1.0012)\tPrec@1 75.781 (75.781)\tPrec@5 89.844 (93.253)\n",
      "Test: [20/391]\tTime 0.773 (1.638)\tLoss 0.9014 (1.0233)\tPrec@1 79.688 (75.781)\tPrec@5 95.312 (93.155)\n",
      "Test: [30/391]\tTime 0.607 (1.564)\tLoss 1.0059 (1.0198)\tPrec@1 75.000 (75.706)\tPrec@5 93.750 (93.196)\n",
      "Test: [40/391]\tTime 0.675 (1.439)\tLoss 1.2031 (0.9880)\tPrec@1 72.656 (76.543)\tPrec@5 91.406 (93.617)\n",
      "Test: [50/391]\tTime 0.521 (1.413)\tLoss 1.2422 (1.0473)\tPrec@1 73.438 (75.398)\tPrec@5 91.406 (93.015)\n",
      "Test: [60/391]\tTime 0.616 (1.350)\tLoss 1.3984 (1.0915)\tPrec@1 65.625 (74.488)\tPrec@5 89.844 (92.533)\n",
      "Test: [70/391]\tTime 4.040 (1.351)\tLoss 1.0420 (1.0838)\tPrec@1 76.562 (74.681)\tPrec@5 92.188 (92.562)\n",
      "Test: [80/391]\tTime 0.716 (1.304)\tLoss 1.0352 (1.0608)\tPrec@1 77.344 (75.183)\tPrec@5 92.969 (92.814)\n",
      "Test: [90/391]\tTime 0.605 (1.276)\tLoss 0.9790 (1.0476)\tPrec@1 75.781 (75.489)\tPrec@5 95.312 (92.986)\n",
      "Test: [100/391]\tTime 0.662 (1.293)\tLoss 0.8789 (1.0282)\tPrec@1 78.906 (75.882)\tPrec@5 94.531 (93.216)\n",
      "Test: [110/391]\tTime 0.608 (1.276)\tLoss 1.2676 (1.0231)\tPrec@1 72.656 (76.014)\tPrec@5 90.625 (93.250)\n",
      "Test: [120/391]\tTime 0.603 (1.275)\tLoss 1.2969 (1.0433)\tPrec@1 74.219 (75.555)\tPrec@5 87.500 (92.898)\n",
      "Test: [130/391]\tTime 0.592 (1.249)\tLoss 0.9375 (1.0327)\tPrec@1 75.781 (75.680)\tPrec@5 91.406 (92.999)\n",
      "Test: [140/391]\tTime 2.845 (1.262)\tLoss 0.8257 (1.0185)\tPrec@1 79.688 (75.870)\tPrec@5 93.750 (93.124)\n",
      "Test: [150/391]\tTime 0.589 (1.250)\tLoss 0.6187 (1.0088)\tPrec@1 84.375 (76.071)\tPrec@5 96.875 (93.181)\n",
      "Test: [160/391]\tTime 1.823 (1.246)\tLoss 0.8687 (0.9941)\tPrec@1 78.906 (76.436)\tPrec@5 92.188 (93.289)\n",
      "Test: [170/391]\tTime 2.596 (1.256)\tLoss 0.7061 (0.9865)\tPrec@1 82.812 (76.636)\tPrec@5 95.312 (93.343)\n",
      "Test: [180/391]\tTime 2.440 (1.264)\tLoss 0.8970 (0.9815)\tPrec@1 77.344 (76.701)\tPrec@5 96.094 (93.362)\n",
      "Test: [190/391]\tTime 0.825 (1.253)\tLoss 0.9033 (0.9769)\tPrec@1 76.562 (76.775)\tPrec@5 94.531 (93.439)\n",
      "Test: [200/391]\tTime 0.552 (1.248)\tLoss 0.7847 (0.9623)\tPrec@1 80.469 (77.099)\tPrec@5 97.656 (93.579)\n",
      "Test: [210/391]\tTime 0.575 (1.241)\tLoss 0.8296 (0.9559)\tPrec@1 74.219 (77.199)\tPrec@5 94.531 (93.683)\n",
      "Test: [220/391]\tTime 4.589 (1.252)\tLoss 0.4062 (0.9489)\tPrec@1 91.406 (77.262)\tPrec@5 98.438 (93.824)\n",
      "Test: [230/391]\tTime 0.687 (1.243)\tLoss 1.2012 (0.9441)\tPrec@1 72.656 (77.398)\tPrec@5 86.719 (93.845)\n",
      "Test: [240/391]\tTime 3.124 (1.249)\tLoss 1.6143 (0.9549)\tPrec@1 65.625 (77.185)\tPrec@5 85.938 (93.734)\n",
      "Test: [250/391]\tTime 2.200 (1.252)\tLoss 0.7969 (0.9597)\tPrec@1 79.688 (77.067)\tPrec@5 95.312 (93.691)\n",
      "Test: [260/391]\tTime 0.741 (1.240)\tLoss 1.2207 (0.9691)\tPrec@1 77.344 (76.922)\tPrec@5 87.500 (93.549)\n",
      "Test: [270/391]\tTime 0.711 (1.244)\tLoss 0.8428 (0.9726)\tPrec@1 78.906 (76.842)\tPrec@5 93.750 (93.542)\n",
      "Test: [280/391]\tTime 0.672 (1.234)\tLoss 1.3604 (0.9813)\tPrec@1 75.000 (76.671)\tPrec@5 85.938 (93.386)\n",
      "Test: [290/391]\tTime 0.623 (1.241)\tLoss 1.3848 (0.9902)\tPrec@1 67.188 (76.493)\tPrec@5 87.500 (93.256)\n",
      "Test: [300/391]\tTime 0.717 (1.233)\tLoss 0.9009 (0.9926)\tPrec@1 78.906 (76.425)\tPrec@5 97.656 (93.252)\n",
      "Test: [310/391]\tTime 5.748 (1.240)\tLoss 1.2979 (0.9924)\tPrec@1 74.219 (76.392)\tPrec@5 91.406 (93.278)\n",
      "Test: [320/391]\tTime 0.704 (1.233)\tLoss 1.0439 (0.9893)\tPrec@1 72.656 (76.414)\tPrec@5 92.188 (93.302)\n",
      "Test: [330/391]\tTime 0.607 (1.226)\tLoss 0.9014 (0.9854)\tPrec@1 77.344 (76.494)\tPrec@5 94.531 (93.342)\n",
      "Test: [340/391]\tTime 0.640 (1.228)\tLoss 0.6855 (0.9834)\tPrec@1 88.281 (76.576)\tPrec@5 96.875 (93.367)\n",
      "Test: [350/391]\tTime 0.597 (1.223)\tLoss 0.9673 (0.9798)\tPrec@1 78.906 (76.663)\tPrec@5 94.531 (93.412)\n",
      "Test: [360/391]\tTime 0.653 (1.232)\tLoss 0.6992 (0.9732)\tPrec@1 83.594 (76.798)\tPrec@5 96.875 (93.486)\n",
      "Test: [370/391]\tTime 0.745 (1.223)\tLoss 1.3203 (0.9737)\tPrec@1 72.656 (76.830)\tPrec@5 87.500 (93.487)\n",
      "Test: [380/391]\tTime 4.108 (1.227)\tLoss 0.6782 (0.9705)\tPrec@1 84.375 (76.903)\tPrec@5 95.312 (93.543)\n",
      "Test: [390/391]\tTime 0.314 (1.208)\tLoss 1.1787 (0.9717)\tPrec@1 73.750 (76.852)\tPrec@5 87.500 (93.536)\n",
      "~~0.1312072625\t93.536\n",
      "\n",
      " * Prec@1 76.852 Prec@5 93.536\n"
     ]
    }
   ],
   "source": [
    "min_scale = 0.5\n",
    "trn_tfms = [\n",
    "        transforms.Resize(int(target_size*1.14)),\n",
    "        transforms.RandomResizedCrop(target_size, scale=(min_scale, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ] + tensor_tfm\n",
    "aug_dataset = datasets.ImageFolder(valdir, transforms.Compose(trn_tfms))\n",
    "\n",
    "val_tfms = [transforms.Resize(int(target_size*1.14)), transforms.CenterCrop(target_size)] + tensor_tfm\n",
    "val_dataset = datasets.ImageFolder(valdir,  transforms.Compose(val_tfms))\n",
    "\n",
    "aug_loader = torch.utils.data.DataLoader(\n",
    "    aug_dataset, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "tta_rs_prec5 = validate(val_loader, model, criterion, aug_loader=aug_loader, num_augmentations=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AR Mean</th>\n",
       "      <th>DawnValidation</th>\n",
       "      <th>TTA</th>\n",
       "      <th>ARValidation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.636959</td>\n",
       "      <td>88.28125</td>\n",
       "      <td>91.40625</td>\n",
       "      <td>92.18750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.667781</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>95.31250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.717153</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>92.96875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>89.06250</td>\n",
       "      <td>91.40625</td>\n",
       "      <td>92.18750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>91.40625</td>\n",
       "      <td>90.62500</td>\n",
       "      <td>91.40625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>88.28125</td>\n",
       "      <td>88.28125</td>\n",
       "      <td>89.84375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.750127</td>\n",
       "      <td>92.18750</td>\n",
       "      <td>92.18750</td>\n",
       "      <td>92.18750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.777394</td>\n",
       "      <td>90.62500</td>\n",
       "      <td>92.96875</td>\n",
       "      <td>92.96875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.830958</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>96.87500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.922669</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>94.53125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.995605</td>\n",
       "      <td>90.62500</td>\n",
       "      <td>90.62500</td>\n",
       "      <td>90.62500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000317</td>\n",
       "      <td>88.28125</td>\n",
       "      <td>88.28125</td>\n",
       "      <td>88.28125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.045130</td>\n",
       "      <td>89.84375</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>91.40625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.125204</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>93.75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.197685</td>\n",
       "      <td>96.09375</td>\n",
       "      <td>96.87500</td>\n",
       "      <td>97.65625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.245955</td>\n",
       "      <td>89.84375</td>\n",
       "      <td>90.62500</td>\n",
       "      <td>89.84375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.275184</td>\n",
       "      <td>93.75000</td>\n",
       "      <td>96.09375</td>\n",
       "      <td>95.31250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.319574</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>96.87500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.333231</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>92.96875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>98.43750</td>\n",
       "      <td>96.87500</td>\n",
       "      <td>94.53125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>93.75000</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>93.75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>96.87500</td>\n",
       "      <td>98.43750</td>\n",
       "      <td>98.43750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>87.50000</td>\n",
       "      <td>87.50000</td>\n",
       "      <td>88.28125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>85.15625</td>\n",
       "      <td>86.71875</td>\n",
       "      <td>86.71875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>96.09375</td>\n",
       "      <td>96.09375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>86.71875</td>\n",
       "      <td>89.06250</td>\n",
       "      <td>89.84375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>94.53125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>87.50000</td>\n",
       "      <td>88.28125</td>\n",
       "      <td>85.93750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>84.37500</td>\n",
       "      <td>86.71875</td>\n",
       "      <td>89.84375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>97.65625</td>\n",
       "      <td>96.87500</td>\n",
       "      <td>96.87500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>89.84375</td>\n",
       "      <td>89.84375</td>\n",
       "      <td>90.62500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.341604</td>\n",
       "      <td>93.75000</td>\n",
       "      <td>92.96875</td>\n",
       "      <td>91.40625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.398043</td>\n",
       "      <td>96.09375</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>95.31250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.465487</td>\n",
       "      <td>96.87500</td>\n",
       "      <td>96.09375</td>\n",
       "      <td>96.09375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.495881</td>\n",
       "      <td>92.18750</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>91.40625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.500512</td>\n",
       "      <td>97.65625</td>\n",
       "      <td>97.65625</td>\n",
       "      <td>97.65625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.501500</td>\n",
       "      <td>85.15625</td>\n",
       "      <td>87.50000</td>\n",
       "      <td>88.28125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.503053</td>\n",
       "      <td>96.09375</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>96.09375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.675084</td>\n",
       "      <td>86.25000</td>\n",
       "      <td>88.75000</td>\n",
       "      <td>92.50000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     AR Mean  DawnValidation       TTA  ARValidation\n",
       "0   0.636959        88.28125  91.40625      92.18750\n",
       "1   0.667781        95.31250  95.31250      95.31250\n",
       "2   0.717153        94.53125  94.53125      92.96875\n",
       "3   0.750000        89.06250  91.40625      92.18750\n",
       "4   0.750000        91.40625  90.62500      91.40625\n",
       "5   0.750000        88.28125  88.28125      89.84375\n",
       "6   0.750127        92.18750  92.18750      92.18750\n",
       "7   0.777394        90.62500  92.96875      92.96875\n",
       "8   0.830958        95.31250  94.53125      96.87500\n",
       "9   0.922669        94.53125  94.53125      94.53125\n",
       "10  0.995605        90.62500  90.62500      90.62500\n",
       "11  1.000317        88.28125  88.28125      88.28125\n",
       "12  1.045130        89.84375  94.53125      91.40625\n",
       "13  1.125204        95.31250  95.31250      93.75000\n",
       "14  1.197685        96.09375  96.87500      97.65625\n",
       "15  1.245955        89.84375  90.62500      89.84375\n",
       "16  1.275184        93.75000  96.09375      95.31250\n",
       "17  1.319574        94.53125  95.31250      96.87500\n",
       "18  1.333231        94.53125  95.31250      92.96875\n",
       "19  1.333330        98.43750  96.87500      94.53125\n",
       "20  1.333330        93.75000  94.53125      93.75000\n",
       "21  1.333330        96.87500  98.43750      98.43750\n",
       "22  1.333330        87.50000  87.50000      88.28125\n",
       "23  1.333330        85.15625  86.71875      86.71875\n",
       "24  1.333330        94.53125  96.09375      96.09375\n",
       "25  1.333330        86.71875  89.06250      89.84375\n",
       "26  1.333330        94.53125  94.53125      94.53125\n",
       "27  1.333330        87.50000  88.28125      85.93750\n",
       "28  1.333330        84.37500  86.71875      89.84375\n",
       "29  1.333330        97.65625  96.87500      96.87500\n",
       "30  1.333330        89.84375  89.84375      90.62500\n",
       "31  1.341604        93.75000  92.96875      91.40625\n",
       "32  1.398043        96.09375  95.31250      95.31250\n",
       "33  1.465487        96.87500  96.09375      96.09375\n",
       "34  1.495881        92.18750  94.53125      91.40625\n",
       "35  1.500512        97.65625  97.65625      97.65625\n",
       "36  1.501500        85.15625  87.50000      88.28125\n",
       "37  1.503053        96.09375  95.31250      96.09375\n",
       "38  1.675084        86.25000  88.75000      92.50000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {\n",
    "    'AR Mean': ar_batch_means[:-1],\n",
    "    'DawnValidation': orig_prec5, \n",
    "    'TTA': tta_prec5, \n",
    "    'ARValidation': ar_rs_prec5, \n",
    "#     'Difference': np.array(ar_rs_prec5)-np.array(orig_prec5)\n",
    "}\n",
    "df = pd.DataFrame(data=d);\n",
    "df.to_csv('ar_tests_tta.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTA with AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_scale = 0.5\n",
    "trn_tfms = [\n",
    "        transforms.RandomResizedCrop(target_size, scale=(min_scale, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ] + tensor_tfm\n",
    "aug_dataset = datasets.ImageFolder(valdir, transforms.Compose(trn_tfms))\n",
    "\n",
    "aug_loader = torch.utils.data.DataLoader(\n",
    "    aug_dataset, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "val_ar_tfms = [transforms.Resize(int(target_size*1.14)), RectangularCropTfm(idx2ar, target_size)]\n",
    "val_dataset_ar_rs = ValDataset(valdir, val_ar_tfms+tensor_tfm)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset_ar_rs, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True, sampler=val_sampler_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [10/391]\tTime 0.600 (3.145)\tLoss 1.1875 (0.9703)\tPrec@1 75.781 (76.278)\tPrec@5 91.406 (94.460)\n",
      "Test: [20/391]\tTime 3.100 (2.190)\tLoss 0.9180 (0.9912)\tPrec@1 78.125 (76.228)\tPrec@5 95.312 (94.085)\n",
      "Test: [30/391]\tTime 0.538 (1.921)\tLoss 0.9868 (0.9960)\tPrec@1 75.000 (76.411)\tPrec@5 92.188 (93.826)\n",
      "Test: [40/391]\tTime 0.696 (1.613)\tLoss 1.2012 (0.9661)\tPrec@1 73.438 (77.115)\tPrec@5 91.406 (94.055)\n",
      "Test: [50/391]\tTime 0.632 (1.436)\tLoss 1.1846 (1.0168)\tPrec@1 74.219 (76.180)\tPrec@5 90.625 (93.413)\n",
      "Test: [60/391]\tTime 2.159 (1.408)\tLoss 1.3584 (1.0595)\tPrec@1 66.406 (75.192)\tPrec@5 89.844 (92.866)\n",
      "Test: [70/391]\tTime 0.551 (1.368)\tLoss 1.0029 (1.0548)\tPrec@1 78.125 (75.330)\tPrec@5 92.188 (92.826)\n",
      "Test: [80/391]\tTime 2.978 (1.358)\tLoss 0.9819 (1.0313)\tPrec@1 77.344 (75.781)\tPrec@5 93.750 (93.046)\n",
      "Test: [90/391]\tTime 0.535 (1.328)\tLoss 0.8921 (1.0211)\tPrec@1 78.906 (76.030)\tPrec@5 96.875 (93.252)\n",
      "Test: [100/391]\tTime 0.567 (1.318)\tLoss 0.8701 (1.0010)\tPrec@1 80.469 (76.408)\tPrec@5 94.531 (93.502)\n",
      "Test: [110/391]\tTime 0.606 (1.254)\tLoss 1.2461 (0.9952)\tPrec@1 71.875 (76.506)\tPrec@5 90.625 (93.525)\n",
      "Test: [120/391]\tTime 0.587 (1.208)\tLoss 1.2871 (1.0142)\tPrec@1 75.000 (76.065)\tPrec@5 88.281 (93.253)\n",
      "Test: [130/391]\tTime 2.952 (1.228)\tLoss 0.9346 (1.0039)\tPrec@1 76.562 (76.163)\tPrec@5 91.406 (93.333)\n",
      "Test: [140/391]\tTime 2.456 (1.233)\tLoss 0.7983 (0.9898)\tPrec@1 82.031 (76.435)\tPrec@5 95.312 (93.456)\n",
      "Test: [150/391]\tTime 0.715 (1.220)\tLoss 0.5771 (0.9802)\tPrec@1 83.594 (76.656)\tPrec@5 96.875 (93.512)\n",
      "Test: [160/391]\tTime 0.571 (1.200)\tLoss 0.8672 (0.9659)\tPrec@1 76.562 (76.994)\tPrec@5 91.406 (93.600)\n",
      "Test: [170/391]\tTime 0.876 (1.181)\tLoss 0.6904 (0.9589)\tPrec@1 83.594 (77.170)\tPrec@5 96.094 (93.645)\n",
      "Test: [180/391]\tTime 0.692 (1.171)\tLoss 0.9326 (0.9548)\tPrec@1 78.125 (77.236)\tPrec@5 96.875 (93.690)\n",
      "Test: [190/391]\tTime 0.648 (1.158)\tLoss 0.9072 (0.9502)\tPrec@1 78.906 (77.344)\tPrec@5 94.531 (93.779)\n",
      "Test: [200/391]\tTime 0.596 (1.152)\tLoss 0.7876 (0.9368)\tPrec@1 79.688 (77.666)\tPrec@5 96.875 (93.913)\n",
      "Test: [210/391]\tTime 2.655 (1.153)\tLoss 0.8452 (0.9319)\tPrec@1 76.562 (77.733)\tPrec@5 94.531 (93.994)\n",
      "Test: [220/391]\tTime 0.579 (1.156)\tLoss 0.3896 (0.9253)\tPrec@1 90.625 (77.782)\tPrec@5 97.656 (94.096)\n",
      "Test: [230/391]\tTime 0.636 (1.145)\tLoss 1.1318 (0.9207)\tPrec@1 76.562 (77.905)\tPrec@5 88.281 (94.105)\n",
      "Test: [240/391]\tTime 0.600 (1.141)\tLoss 1.5996 (0.9316)\tPrec@1 68.750 (77.671)\tPrec@5 87.500 (94.003)\n",
      "Test: [250/391]\tTime 0.721 (1.136)\tLoss 0.7432 (0.9362)\tPrec@1 79.688 (77.562)\tPrec@5 96.094 (93.977)\n",
      "Test: [260/391]\tTime 0.744 (1.136)\tLoss 1.1631 (0.9455)\tPrec@1 76.562 (77.422)\tPrec@5 88.281 (93.825)\n",
      "Test: [270/391]\tTime 0.636 (1.136)\tLoss 0.8657 (0.9500)\tPrec@1 83.594 (77.312)\tPrec@5 92.969 (93.808)\n",
      "Test: [280/391]\tTime 3.548 (1.129)\tLoss 1.2939 (0.9585)\tPrec@1 75.781 (77.180)\tPrec@5 85.938 (93.642)\n",
      "Test: [290/391]\tTime 0.676 (1.121)\tLoss 1.3926 (0.9673)\tPrec@1 66.406 (76.979)\tPrec@5 89.844 (93.543)\n",
      "Test: [300/391]\tTime 0.568 (1.117)\tLoss 0.9141 (0.9696)\tPrec@1 76.562 (76.918)\tPrec@5 96.875 (93.537)\n",
      "Test: [310/391]\tTime 0.578 (1.118)\tLoss 1.2578 (0.9696)\tPrec@1 72.656 (76.884)\tPrec@5 90.625 (93.531)\n",
      "Test: [320/391]\tTime 0.528 (1.120)\tLoss 1.0273 (0.9666)\tPrec@1 69.531 (76.889)\tPrec@5 92.188 (93.550)\n",
      "Test: [330/391]\tTime 0.542 (1.116)\tLoss 0.8760 (0.9631)\tPrec@1 76.562 (76.954)\tPrec@5 94.531 (93.599)\n",
      "Test: [340/391]\tTime 0.603 (1.113)\tLoss 0.6636 (0.9616)\tPrec@1 89.062 (77.018)\tPrec@5 96.875 (93.622)\n",
      "Test: [350/391]\tTime 0.920 (1.103)\tLoss 0.9360 (0.9584)\tPrec@1 79.688 (77.090)\tPrec@5 95.312 (93.663)\n",
      "Test: [360/391]\tTime 0.556 (1.102)\tLoss 0.7397 (0.9527)\tPrec@1 82.031 (77.194)\tPrec@5 97.656 (93.741)\n",
      "Test: [370/391]\tTime 0.693 (1.095)\tLoss 1.3223 (0.9533)\tPrec@1 71.875 (77.194)\tPrec@5 88.281 (93.744)\n",
      "Test: [380/391]\tTime 0.729 (1.093)\tLoss 0.6787 (0.9507)\tPrec@1 84.375 (77.268)\tPrec@5 96.094 (93.791)\n",
      "Test: [390/391]\tTime 5.619 (1.144)\tLoss 1.1045 (0.9522)\tPrec@1 73.750 (77.222)\tPrec@5 91.250 (93.788)\n",
      "~~0.12421399777777778\t93.788\n",
      "\n",
      " * Prec@1 77.222 Prec@5 93.788\n"
     ]
    }
   ],
   "source": [
    "tta_ar_rs_prec5 = validate(val_loader, model, criterion, aug_loader=aug_loader, num_augmentations=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AR Mean</th>\n",
       "      <th>DawnValidation</th>\n",
       "      <th>TTA</th>\n",
       "      <th>ARValidation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.636959</td>\n",
       "      <td>88.28125</td>\n",
       "      <td>91.40625</td>\n",
       "      <td>92.18750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.667781</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>95.31250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.717153</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>92.96875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>89.06250</td>\n",
       "      <td>91.40625</td>\n",
       "      <td>92.18750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>91.40625</td>\n",
       "      <td>90.62500</td>\n",
       "      <td>91.40625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>88.28125</td>\n",
       "      <td>88.28125</td>\n",
       "      <td>89.84375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.750127</td>\n",
       "      <td>92.18750</td>\n",
       "      <td>92.18750</td>\n",
       "      <td>92.18750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.777394</td>\n",
       "      <td>90.62500</td>\n",
       "      <td>92.96875</td>\n",
       "      <td>92.96875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.830958</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>96.87500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.922669</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>94.53125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.995605</td>\n",
       "      <td>90.62500</td>\n",
       "      <td>90.62500</td>\n",
       "      <td>90.62500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000317</td>\n",
       "      <td>88.28125</td>\n",
       "      <td>88.28125</td>\n",
       "      <td>88.28125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.045130</td>\n",
       "      <td>89.84375</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>91.40625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.125204</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>93.75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.197685</td>\n",
       "      <td>96.09375</td>\n",
       "      <td>96.87500</td>\n",
       "      <td>97.65625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.245955</td>\n",
       "      <td>89.84375</td>\n",
       "      <td>90.62500</td>\n",
       "      <td>89.84375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.275184</td>\n",
       "      <td>93.75000</td>\n",
       "      <td>96.09375</td>\n",
       "      <td>95.31250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.319574</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>96.87500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.333231</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>92.96875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>98.43750</td>\n",
       "      <td>96.87500</td>\n",
       "      <td>94.53125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>93.75000</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>93.75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>96.87500</td>\n",
       "      <td>98.43750</td>\n",
       "      <td>98.43750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>87.50000</td>\n",
       "      <td>87.50000</td>\n",
       "      <td>88.28125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>85.15625</td>\n",
       "      <td>86.71875</td>\n",
       "      <td>86.71875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>96.09375</td>\n",
       "      <td>96.09375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>86.71875</td>\n",
       "      <td>89.06250</td>\n",
       "      <td>89.84375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>94.53125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>87.50000</td>\n",
       "      <td>88.28125</td>\n",
       "      <td>85.93750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>84.37500</td>\n",
       "      <td>86.71875</td>\n",
       "      <td>89.84375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>97.65625</td>\n",
       "      <td>96.87500</td>\n",
       "      <td>96.87500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>89.84375</td>\n",
       "      <td>89.84375</td>\n",
       "      <td>90.62500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.341604</td>\n",
       "      <td>93.75000</td>\n",
       "      <td>92.96875</td>\n",
       "      <td>91.40625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.398043</td>\n",
       "      <td>96.09375</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>95.31250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.465487</td>\n",
       "      <td>96.87500</td>\n",
       "      <td>96.09375</td>\n",
       "      <td>96.09375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.495881</td>\n",
       "      <td>92.18750</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>91.40625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.500512</td>\n",
       "      <td>97.65625</td>\n",
       "      <td>97.65625</td>\n",
       "      <td>97.65625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.501500</td>\n",
       "      <td>85.15625</td>\n",
       "      <td>87.50000</td>\n",
       "      <td>88.28125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.503053</td>\n",
       "      <td>96.09375</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>96.09375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.675084</td>\n",
       "      <td>86.25000</td>\n",
       "      <td>88.75000</td>\n",
       "      <td>92.50000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     AR Mean  DawnValidation       TTA  ARValidation\n",
       "0   0.636959        88.28125  91.40625      92.18750\n",
       "1   0.667781        95.31250  95.31250      95.31250\n",
       "2   0.717153        94.53125  94.53125      92.96875\n",
       "3   0.750000        89.06250  91.40625      92.18750\n",
       "4   0.750000        91.40625  90.62500      91.40625\n",
       "5   0.750000        88.28125  88.28125      89.84375\n",
       "6   0.750127        92.18750  92.18750      92.18750\n",
       "7   0.777394        90.62500  92.96875      92.96875\n",
       "8   0.830958        95.31250  94.53125      96.87500\n",
       "9   0.922669        94.53125  94.53125      94.53125\n",
       "10  0.995605        90.62500  90.62500      90.62500\n",
       "11  1.000317        88.28125  88.28125      88.28125\n",
       "12  1.045130        89.84375  94.53125      91.40625\n",
       "13  1.125204        95.31250  95.31250      93.75000\n",
       "14  1.197685        96.09375  96.87500      97.65625\n",
       "15  1.245955        89.84375  90.62500      89.84375\n",
       "16  1.275184        93.75000  96.09375      95.31250\n",
       "17  1.319574        94.53125  95.31250      96.87500\n",
       "18  1.333231        94.53125  95.31250      92.96875\n",
       "19  1.333330        98.43750  96.87500      94.53125\n",
       "20  1.333330        93.75000  94.53125      93.75000\n",
       "21  1.333330        96.87500  98.43750      98.43750\n",
       "22  1.333330        87.50000  87.50000      88.28125\n",
       "23  1.333330        85.15625  86.71875      86.71875\n",
       "24  1.333330        94.53125  96.09375      96.09375\n",
       "25  1.333330        86.71875  89.06250      89.84375\n",
       "26  1.333330        94.53125  94.53125      94.53125\n",
       "27  1.333330        87.50000  88.28125      85.93750\n",
       "28  1.333330        84.37500  86.71875      89.84375\n",
       "29  1.333330        97.65625  96.87500      96.87500\n",
       "30  1.333330        89.84375  89.84375      90.62500\n",
       "31  1.341604        93.75000  92.96875      91.40625\n",
       "32  1.398043        96.09375  95.31250      95.31250\n",
       "33  1.465487        96.87500  96.09375      96.09375\n",
       "34  1.495881        92.18750  94.53125      91.40625\n",
       "35  1.500512        97.65625  97.65625      97.65625\n",
       "36  1.501500        85.15625  87.50000      88.28125\n",
       "37  1.503053        96.09375  95.31250      96.09375\n",
       "38  1.675084        86.25000  88.75000      92.50000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ar_tests_tta.csv', index_col=0); df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.DataFrame({'TTA_AR': tta_ar_rs_prec5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ar_tests_tta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AR Mean</th>\n",
       "      <th>DawnValidation</th>\n",
       "      <th>TTA</th>\n",
       "      <th>ARValidation</th>\n",
       "      <th>TTA_AR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.636959</td>\n",
       "      <td>88.28125</td>\n",
       "      <td>91.40625</td>\n",
       "      <td>92.18750</td>\n",
       "      <td>91.40625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.667781</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>95.31250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.717153</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>92.96875</td>\n",
       "      <td>92.18750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>89.06250</td>\n",
       "      <td>91.40625</td>\n",
       "      <td>92.18750</td>\n",
       "      <td>91.40625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>91.40625</td>\n",
       "      <td>90.62500</td>\n",
       "      <td>91.40625</td>\n",
       "      <td>90.62500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>88.28125</td>\n",
       "      <td>88.28125</td>\n",
       "      <td>89.84375</td>\n",
       "      <td>89.84375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.750127</td>\n",
       "      <td>92.18750</td>\n",
       "      <td>92.18750</td>\n",
       "      <td>92.18750</td>\n",
       "      <td>92.18750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.777394</td>\n",
       "      <td>90.62500</td>\n",
       "      <td>92.96875</td>\n",
       "      <td>92.96875</td>\n",
       "      <td>93.75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.830958</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>96.87500</td>\n",
       "      <td>96.87500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.922669</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>94.53125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.995605</td>\n",
       "      <td>90.62500</td>\n",
       "      <td>90.62500</td>\n",
       "      <td>90.62500</td>\n",
       "      <td>90.62500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000317</td>\n",
       "      <td>88.28125</td>\n",
       "      <td>88.28125</td>\n",
       "      <td>88.28125</td>\n",
       "      <td>88.28125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.045130</td>\n",
       "      <td>89.84375</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>91.40625</td>\n",
       "      <td>91.40625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.125204</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>93.75000</td>\n",
       "      <td>95.31250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.197685</td>\n",
       "      <td>96.09375</td>\n",
       "      <td>96.87500</td>\n",
       "      <td>97.65625</td>\n",
       "      <td>96.87500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.245955</td>\n",
       "      <td>89.84375</td>\n",
       "      <td>90.62500</td>\n",
       "      <td>89.84375</td>\n",
       "      <td>91.40625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.275184</td>\n",
       "      <td>93.75000</td>\n",
       "      <td>96.09375</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>96.09375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.319574</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>96.87500</td>\n",
       "      <td>96.87500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.333231</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>92.96875</td>\n",
       "      <td>94.53125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>98.43750</td>\n",
       "      <td>96.87500</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>96.87500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>93.75000</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>93.75000</td>\n",
       "      <td>94.53125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>96.87500</td>\n",
       "      <td>98.43750</td>\n",
       "      <td>98.43750</td>\n",
       "      <td>97.65625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>87.50000</td>\n",
       "      <td>87.50000</td>\n",
       "      <td>88.28125</td>\n",
       "      <td>88.28125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>85.15625</td>\n",
       "      <td>86.71875</td>\n",
       "      <td>86.71875</td>\n",
       "      <td>87.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>96.09375</td>\n",
       "      <td>96.09375</td>\n",
       "      <td>96.09375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>86.71875</td>\n",
       "      <td>89.06250</td>\n",
       "      <td>89.84375</td>\n",
       "      <td>88.28125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>92.96875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>87.50000</td>\n",
       "      <td>88.28125</td>\n",
       "      <td>85.93750</td>\n",
       "      <td>85.93750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>84.37500</td>\n",
       "      <td>86.71875</td>\n",
       "      <td>89.84375</td>\n",
       "      <td>89.84375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>97.65625</td>\n",
       "      <td>96.87500</td>\n",
       "      <td>96.87500</td>\n",
       "      <td>96.87500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.333330</td>\n",
       "      <td>89.84375</td>\n",
       "      <td>89.84375</td>\n",
       "      <td>90.62500</td>\n",
       "      <td>90.62500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.341604</td>\n",
       "      <td>93.75000</td>\n",
       "      <td>92.96875</td>\n",
       "      <td>91.40625</td>\n",
       "      <td>92.18750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.398043</td>\n",
       "      <td>96.09375</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>94.53125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.465487</td>\n",
       "      <td>96.87500</td>\n",
       "      <td>96.09375</td>\n",
       "      <td>96.09375</td>\n",
       "      <td>96.87500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.495881</td>\n",
       "      <td>92.18750</td>\n",
       "      <td>94.53125</td>\n",
       "      <td>91.40625</td>\n",
       "      <td>95.31250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.500512</td>\n",
       "      <td>97.65625</td>\n",
       "      <td>97.65625</td>\n",
       "      <td>97.65625</td>\n",
       "      <td>97.65625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.501500</td>\n",
       "      <td>85.15625</td>\n",
       "      <td>87.50000</td>\n",
       "      <td>88.28125</td>\n",
       "      <td>88.28125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.503053</td>\n",
       "      <td>96.09375</td>\n",
       "      <td>95.31250</td>\n",
       "      <td>96.09375</td>\n",
       "      <td>96.09375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.675084</td>\n",
       "      <td>86.25000</td>\n",
       "      <td>88.75000</td>\n",
       "      <td>92.50000</td>\n",
       "      <td>91.25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     AR Mean  DawnValidation       TTA  ARValidation    TTA_AR\n",
       "0   0.636959        88.28125  91.40625      92.18750  91.40625\n",
       "1   0.667781        95.31250  95.31250      95.31250  95.31250\n",
       "2   0.717153        94.53125  94.53125      92.96875  92.18750\n",
       "3   0.750000        89.06250  91.40625      92.18750  91.40625\n",
       "4   0.750000        91.40625  90.62500      91.40625  90.62500\n",
       "5   0.750000        88.28125  88.28125      89.84375  89.84375\n",
       "6   0.750127        92.18750  92.18750      92.18750  92.18750\n",
       "7   0.777394        90.62500  92.96875      92.96875  93.75000\n",
       "8   0.830958        95.31250  94.53125      96.87500  96.87500\n",
       "9   0.922669        94.53125  94.53125      94.53125  94.53125\n",
       "10  0.995605        90.62500  90.62500      90.62500  90.62500\n",
       "11  1.000317        88.28125  88.28125      88.28125  88.28125\n",
       "12  1.045130        89.84375  94.53125      91.40625  91.40625\n",
       "13  1.125204        95.31250  95.31250      93.75000  95.31250\n",
       "14  1.197685        96.09375  96.87500      97.65625  96.87500\n",
       "15  1.245955        89.84375  90.62500      89.84375  91.40625\n",
       "16  1.275184        93.75000  96.09375      95.31250  96.09375\n",
       "17  1.319574        94.53125  95.31250      96.87500  96.87500\n",
       "18  1.333231        94.53125  95.31250      92.96875  94.53125\n",
       "19  1.333330        98.43750  96.87500      94.53125  96.87500\n",
       "20  1.333330        93.75000  94.53125      93.75000  94.53125\n",
       "21  1.333330        96.87500  98.43750      98.43750  97.65625\n",
       "22  1.333330        87.50000  87.50000      88.28125  88.28125\n",
       "23  1.333330        85.15625  86.71875      86.71875  87.50000\n",
       "24  1.333330        94.53125  96.09375      96.09375  96.09375\n",
       "25  1.333330        86.71875  89.06250      89.84375  88.28125\n",
       "26  1.333330        94.53125  94.53125      94.53125  92.96875\n",
       "27  1.333330        87.50000  88.28125      85.93750  85.93750\n",
       "28  1.333330        84.37500  86.71875      89.84375  89.84375\n",
       "29  1.333330        97.65625  96.87500      96.87500  96.87500\n",
       "30  1.333330        89.84375  89.84375      90.62500  90.62500\n",
       "31  1.341604        93.75000  92.96875      91.40625  92.18750\n",
       "32  1.398043        96.09375  95.31250      95.31250  94.53125\n",
       "33  1.465487        96.87500  96.09375      96.09375  96.87500\n",
       "34  1.495881        92.18750  94.53125      91.40625  95.31250\n",
       "35  1.500512        97.65625  97.65625      97.65625  97.65625\n",
       "36  1.501500        85.15625  87.50000      88.28125  88.28125\n",
       "37  1.503053        96.09375  95.31250      96.09375  96.09375\n",
       "38  1.675084        86.25000  88.75000      92.50000  91.25000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.join(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1 == pd pd..DataFrameDataFrame({({'dat1''dat1'::  [[99,,55]})]})\n",
    "dat2 == pd pd..DataFrameDataFrame({({'dat2''dat2'::  [[77,,66]})]})\n",
    " >> dat1 dat1..joinjoin((dat2dat2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
