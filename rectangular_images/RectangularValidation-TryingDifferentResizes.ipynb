{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we compare the different validation techniques on an imagenet model already trained to 93%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, shutil, time, warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import Sampler\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "data = Path.home()/'data/imagenet'\n",
    "workers = 7\n",
    "valdir = os.path.join(data, 'validation')\n",
    "batch_size = 64\n",
    "fp16 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Image to Aspect ratio mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: sort images by aspect ratio\n",
    "def sort_ar(valdir):\n",
    "    idx2ar_file = data/'sorted_idxar.p'\n",
    "    if os.path.isfile(idx2ar_file): return pickle.load(open(idx2ar_file, 'rb'))\n",
    "    print('Creating AR indexes. Please be patient this may take a couple minutes...')\n",
    "    val_dataset = datasets.ImageFolder(valdir)\n",
    "    sizes = [img[0].size for img in tqdm(val_dataset, total=len(val_dataset))]\n",
    "    idx_ar = [(i, round(s[0]/s[1], 5)) for i,s in enumerate(sizes)]\n",
    "    sorted_idxar = sorted(idx_ar, key=lambda x: x[1])\n",
    "    pickle.dump(sorted_idxar, open(idx2ar_file, 'wb'))\n",
    "    return sorted_idxar\n",
    "\n",
    "# Step 2: chunk images by batch size. This way we can crop each image to the batch aspect ratio mean \n",
    "def chunks(l, n):\n",
    "    n = max(1, n)\n",
    "    return (l[i:i+n] for i in range(0, len(l), n))\n",
    "\n",
    "# Step 3: map image index to batch aspect ratio mean so our transform function knows where to crop\n",
    "def map_idx2ar(idx_ar_sorted, batch_size):\n",
    "    ar_chunks = list(chunks(idx_ar_sorted, batch_size))\n",
    "    idx2ar = {}\n",
    "    ar_means = []\n",
    "    for chunk in ar_chunks:\n",
    "        idxs, ars = list(zip(*chunk))\n",
    "        mean = round(np.mean(ars), 5)\n",
    "        ar_means.append(mean)\n",
    "        for idx in idxs:\n",
    "            idx2ar[idx] = mean\n",
    "    return idx2ar, ar_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_ar_sorted = sort_ar(valdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OR just download it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2ar_path = data/'sorted_idxar.p'\n",
    "url = 'https://s3-us-west-2.amazonaws.com/ashaw-fastai-imagenet/sorted_idxar.p'\n",
    "if not os.path.exists(idx2ar_path): urllib.request.urlretrieve(url, idx2ar_path)\n",
    "idx_ar_sorted = sort_ar(valdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValDataset(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None, target_transform=None):\n",
    "        super().__init__(root, transform, target_transform)\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.imgs[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            for tfm in self.transform:\n",
    "                if isinstance(tfm, RectangularCropTfm): sample = tfm(sample, index)\n",
    "                else: sample = tfm(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return sample, target\n",
    "\n",
    "    \n",
    "# Essentially a sequential sampler\n",
    "class SequentialIndexSampler(Sampler):\n",
    "    def __init__(self, indices): self.indices = indices\n",
    "    def __len__(self): return len(self.indices)\n",
    "    def __iter__(self): return iter(self.indices)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RectangularCropTfm(object):\n",
    "    def __init__(self, idx2ar, target_size):\n",
    "        self.idx2ar, self.target_size = idx2ar, target_size\n",
    "    def __call__(self, img, idx):\n",
    "        target_ar = self.idx2ar[idx]\n",
    "        if target_ar < 1: \n",
    "            w = int(self.target_size/target_ar)\n",
    "            size = (w//8*8, self.target_size)\n",
    "        else: \n",
    "            h = int(self.target_size*target_ar)\n",
    "            size = (self.target_size, h//8*8)\n",
    "        return transforms.functional.center_crop(img, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Function with TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, aug_loader=None, num_augmentations=0):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "\n",
    "    val_iter = iter(val_loader)\n",
    "    aug_iters = [iter(aug_loader) for i in range(num_augmentations)]\n",
    "    prec5_arr = []\n",
    "    for i in range(len(val_loader)):\n",
    "        def get_output(dl_iter):\n",
    "            input,target = next(dl_iter)\n",
    "            input, target = input.cuda(), target.cuda()\n",
    "            if fp16: input = input.half()\n",
    "\n",
    "            # compute output\n",
    "            with torch.no_grad():\n",
    "                output = model(Variable(input))\n",
    "                loss = criterion(output, Variable(target))\n",
    "            return output, loss, input, target\n",
    "        \n",
    "        # Normal Validation\n",
    "        output,loss,input,target = get_output(val_iter)\n",
    "        \n",
    "        # TTA\n",
    "        for aug_iter in aug_iters:\n",
    "            o,l,_,_ = get_output(aug_iter)\n",
    "            output.add_(o)\n",
    "            loss.add_(l)\n",
    "        loss.div_(num_augmentations+1)\n",
    "        \n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        reduced_loss = loss.data\n",
    "            \n",
    "        losses.update(to_python_float(reduced_loss), input.size(0))\n",
    "        top1.update(to_python_float(prec1), input.size(0))\n",
    "        top5.update(to_python_float(prec5), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if ((i+1)%50 == 0) or ((i+1) == len(val_loader)):\n",
    "            prec5_arr.append(top5.val)\n",
    "            output = ('Test: [{0}/{1}]\\t' \\\n",
    "                    + 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t' \\\n",
    "                    + 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t' \\\n",
    "                    + 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t' \\\n",
    "                    + 'Prec@5 {top5.val:.3f} ({top5.avg:.3f})').format(\n",
    "                    i+1, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                    top1=top1, top5=top5)\n",
    "            print(output)\n",
    "\n",
    "    time_diff = datetime.now()-start_time\n",
    "    print(f'Total Time:{float(time_diff.total_seconds() / 3600.0)}\\t Top 5 Accuracy: {top5.avg:.3f}\\n')\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n",
    "\n",
    "    return prec5_arr\n",
    "\n",
    "# item() is a recent addition, so this helps with backward compatibility.\n",
    "def to_python_float(t):\n",
    "    if hasattr(t, 'item'): return t.item()\n",
    "    else: return t[0]\n",
    "    \n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self): self.reset()\n",
    "    def reset(self): self.val = self.avg = self.sum = self.count = 0\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get pretrained resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resnet\n",
    "model = resnet.resnet50(pretrained=True)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "if fp16:\n",
    "    from fp16util import network_to_half\n",
    "    model = network_to_half(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset that supports Rectangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValDataset(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None, target_transform=None):\n",
    "        super().__init__(root, transform, target_transform)\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.imgs[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            for tfm in self.transform:\n",
    "                if isinstance(tfm, RectangularCropTfm): sample = tfm(sample, index)\n",
    "                else: sample = tfm(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return sample, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global dataset settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_bs = 64\n",
    "target_size = 288\n",
    "\n",
    "idx_sorted, _ = zip(*idx_ar_sorted)\n",
    "idx2ar, ar_means = map_idx2ar(idx_ar_sorted, val_bs)\n",
    "val_sampler_ar = SequentialIndexSampler(idx_sorted)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "tensor_tfm = [transforms.ToTensor(), normalize]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Original Validation Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the validation technique used in fast.ai's original [DAWNBenchmark](https://dawn.cs.stanford.edu/benchmark/ImageNet/train.html)  \n",
    "Resize Image 1.14x -> Crop to target size (288)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [50/391]\tTime 0.107 (0.234)\tLoss 1.2314 (1.0281)\tPrec@1 72.656 (74.433)\tPrec@5 90.625 (92.019)\n",
      "Test: [100/391]\tTime 0.106 (0.215)\tLoss 0.8887 (0.9856)\tPrec@1 75.781 (75.433)\tPrec@5 93.750 (92.605)\n",
      "Test: [150/391]\tTime 0.107 (0.211)\tLoss 0.6523 (0.9521)\tPrec@1 83.594 (76.247)\tPrec@5 96.094 (93.031)\n",
      "Test: [200/391]\tTime 0.124 (0.209)\tLoss 0.7725 (0.9011)\tPrec@1 82.031 (77.484)\tPrec@5 96.094 (93.528)\n",
      "Test: [250/391]\tTime 0.229 (0.207)\tLoss 0.7354 (0.9047)\tPrec@1 81.250 (77.344)\tPrec@5 94.531 (93.657)\n",
      "Test: [300/391]\tTime 0.120 (0.206)\tLoss 0.8486 (0.9344)\tPrec@1 79.688 (76.806)\tPrec@5 96.875 (93.218)\n",
      "Test: [350/391]\tTime 0.443 (0.207)\tLoss 0.9238 (0.9248)\tPrec@1 77.344 (76.945)\tPrec@5 89.062 (93.354)\n",
      "~~0.022758550833333335\t93.430\n",
      "\n",
      " * Prec@1 76.914 Prec@5 93.430\n"
     ]
    }
   ],
   "source": [
    "val_tfms = [transforms.Resize(int(target_size*1.14)), transforms.CenterCrop(target_size)] + tensor_tfm\n",
    "val_dataset = datasets.ImageFolder(valdir,  transforms.Compose(val_tfms))\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "orig_prec5 = validate(val_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Fast.Ai Rectangular Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size 128 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [50/391]\tTime 0.138 (0.681)\tLoss 1.0703 (1.0479)\tPrec@1 70.312 (76.109)\tPrec@5 91.406 (93.328)\n",
      "Test: [100/391]\tTime 0.126 (0.626)\tLoss 0.7603 (1.0027)\tPrec@1 75.781 (76.578)\tPrec@5 98.438 (93.477)\n",
      "Test: [150/391]\tTime 1.666 (0.558)\tLoss 0.9561 (0.9680)\tPrec@1 75.000 (76.932)\tPrec@5 92.969 (93.656)\n",
      "Test: [200/391]\tTime 0.509 (0.489)\tLoss 0.8340 (0.9238)\tPrec@1 82.031 (77.902)\tPrec@5 96.094 (94.039)\n",
      "Test: [250/391]\tTime 0.142 (0.440)\tLoss 0.8906 (0.9364)\tPrec@1 78.906 (77.703)\tPrec@5 93.750 (94.025)\n",
      "Test: [300/391]\tTime 0.149 (0.403)\tLoss 1.0117 (0.9608)\tPrec@1 79.688 (77.180)\tPrec@5 92.188 (93.667)\n",
      "Test: [350/391]\tTime 0.162 (0.404)\tLoss 0.7207 (0.9545)\tPrec@1 82.812 (77.299)\tPrec@5 96.875 (93.821)\n",
      "Test: [391/391]\tTime 3.615 (0.449)\tLoss 1.0947 (0.9550)\tPrec@1 76.250 (77.352)\tPrec@5 90.000 (93.886)\n",
      "Total Time:0.04871557305555555\t Top 5 Accuracy: 93.886\n",
      "\n",
      " * Prec@1 77.352 Prec@5 93.886\n"
     ]
    }
   ],
   "source": [
    "val_ar_tfms = [transforms.Resize(int(target_size*1.14)), RectangularCropTfm(idx2ar, target_size)]\n",
    "val_dataset_ar_rs = ValDataset(valdir, val_ar_tfms+tensor_tfm)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset_ar_rs, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "ar_rs_prec5 = validate(val_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size 64 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP16 (BN is also fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [50/782]\tTime 0.074 (0.513)\tLoss 1.2061 (1.0429)\tPrec@1 67.188 (76.031)\tPrec@5 90.625 (94.000)\n",
      "Test: [100/782]\tTime 0.125 (0.327)\tLoss 1.4004 (1.0478)\tPrec@1 59.375 (76.172)\tPrec@5 87.500 (93.422)\n",
      "Test: [150/782]\tTime 0.069 (0.267)\tLoss 0.7969 (1.0624)\tPrec@1 81.250 (75.604)\tPrec@5 95.312 (92.948)\n",
      "Test: [200/782]\tTime 0.058 (0.267)\tLoss 0.5542 (1.0024)\tPrec@1 85.938 (76.594)\tPrec@5 100.000 (93.539)\n",
      "Test: [250/782]\tTime 0.067 (0.240)\tLoss 0.7871 (0.9978)\tPrec@1 81.250 (76.412)\tPrec@5 93.750 (93.481)\n",
      "Test: [300/782]\tTime 0.075 (0.237)\tLoss 0.9624 (0.9680)\tPrec@1 76.562 (76.948)\tPrec@5 92.188 (93.698)\n",
      "Test: [350/782]\tTime 0.071 (0.224)\tLoss 0.8110 (0.9422)\tPrec@1 85.938 (77.589)\tPrec@5 95.312 (93.902)\n",
      "Test: [400/782]\tTime 0.071 (0.208)\tLoss 0.9170 (0.9237)\tPrec@1 79.688 (77.914)\tPrec@5 93.750 (94.078)\n",
      "Test: [450/782]\tTime 0.286 (0.198)\tLoss 1.1611 (0.9234)\tPrec@1 64.062 (78.049)\tPrec@5 92.188 (94.243)\n",
      "Test: [500/782]\tTime 0.075 (0.189)\tLoss 0.3828 (0.9363)\tPrec@1 90.625 (77.713)\tPrec@5 98.438 (94.056)\n",
      "Test: [550/782]\tTime 0.077 (0.182)\tLoss 1.3428 (0.9474)\tPrec@1 64.062 (77.480)\tPrec@5 95.312 (93.901)\n",
      "Test: [600/782]\tTime 0.071 (0.175)\tLoss 1.2441 (0.9607)\tPrec@1 70.312 (77.188)\tPrec@5 93.750 (93.693)\n",
      "Test: [650/782]\tTime 0.081 (0.174)\tLoss 0.8921 (0.9572)\tPrec@1 79.688 (77.180)\tPrec@5 95.312 (93.762)\n",
      "Test: [700/782]\tTime 0.080 (0.173)\tLoss 0.5312 (0.9544)\tPrec@1 85.938 (77.304)\tPrec@5 95.312 (93.846)\n",
      "Test: [750/782]\tTime 0.088 (0.169)\tLoss 0.6396 (0.9550)\tPrec@1 81.250 (77.327)\tPrec@5 98.438 (93.877)\n",
      "Test: [782/782]\tTime 1.199 (0.202)\tLoss 0.8638 (0.9548)\tPrec@1 75.000 (77.354)\tPrec@5 93.750 (93.914)\n",
      "Total Time:0.04391798472222223\t Top 5 Accuracy: 93.914\n",
      "\n",
      " * Prec@1 77.354 Prec@5 93.914\n"
     ]
    }
   ],
   "source": [
    "val_ar_tfms = [transforms.Resize(int(target_size*1.14)), RectangularCropTfm(idx2ar, target_size)]\n",
    "val_dataset_ar_rs = ValDataset(valdir, val_ar_tfms+tensor_tfm)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset_ar_rs, batch_size=64, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "ar_rs_prec5 = validate(val_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network to half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [50/782]\tTime 0.069 (0.523)\tLoss 1.2061 (1.0429)\tPrec@1 67.188 (76.062)\tPrec@5 90.625 (94.000)\n",
      "Test: [100/782]\tTime 0.209 (0.335)\tLoss 1.3994 (1.0478)\tPrec@1 59.375 (76.172)\tPrec@5 87.500 (93.406)\n",
      "Test: [150/782]\tTime 0.064 (0.270)\tLoss 0.7974 (1.0625)\tPrec@1 81.250 (75.604)\tPrec@5 95.312 (92.938)\n",
      "Test: [200/782]\tTime 0.069 (0.268)\tLoss 0.5547 (1.0025)\tPrec@1 85.938 (76.594)\tPrec@5 100.000 (93.531)\n",
      "Test: [250/782]\tTime 0.062 (0.243)\tLoss 0.7871 (0.9978)\tPrec@1 81.250 (76.412)\tPrec@5 93.750 (93.469)\n",
      "Test: [300/782]\tTime 0.072 (0.238)\tLoss 0.9624 (0.9680)\tPrec@1 76.562 (76.953)\tPrec@5 92.188 (93.688)\n",
      "Test: [350/782]\tTime 0.065 (0.225)\tLoss 0.8110 (0.9422)\tPrec@1 85.938 (77.594)\tPrec@5 95.312 (93.893)\n",
      "Test: [400/782]\tTime 0.071 (0.210)\tLoss 0.9170 (0.9237)\tPrec@1 79.688 (77.918)\tPrec@5 93.750 (94.070)\n",
      "Test: [450/782]\tTime 0.343 (0.199)\tLoss 1.1611 (0.9234)\tPrec@1 64.062 (78.049)\tPrec@5 92.188 (94.236)\n",
      "Test: [500/782]\tTime 0.065 (0.190)\tLoss 0.3823 (0.9363)\tPrec@1 90.625 (77.709)\tPrec@5 98.438 (94.050)\n",
      "Test: [550/782]\tTime 0.065 (0.183)\tLoss 1.3428 (0.9474)\tPrec@1 64.062 (77.474)\tPrec@5 95.312 (93.895)\n",
      "Test: [600/782]\tTime 0.090 (0.178)\tLoss 1.2441 (0.9607)\tPrec@1 70.312 (77.182)\tPrec@5 93.750 (93.688)\n",
      "Test: [650/782]\tTime 0.082 (0.177)\tLoss 0.8916 (0.9571)\tPrec@1 78.125 (77.178)\tPrec@5 95.312 (93.757)\n",
      "Test: [700/782]\tTime 0.277 (0.175)\tLoss 0.5312 (0.9544)\tPrec@1 85.938 (77.299)\tPrec@5 95.312 (93.842)\n",
      "Test: [750/782]\tTime 0.074 (0.171)\tLoss 0.6396 (0.9550)\tPrec@1 81.250 (77.325)\tPrec@5 98.438 (93.871)\n",
      "Test: [782/782]\tTime 0.977 (0.204)\tLoss 0.8633 (0.9548)\tPrec@1 75.000 (77.356)\tPrec@5 93.750 (93.908)\n",
      "Total Time:0.04429139361111111\t Top 5 Accuracy: 93.908\n",
      "\n",
      " * Prec@1 77.356 Prec@5 93.908\n"
     ]
    }
   ],
   "source": [
    "val_ar_tfms = [transforms.Resize(int(target_size*1.14)), RectangularCropTfm(idx2ar, target_size)]\n",
    "val_dataset_ar_rs = ValDataset(valdir, val_ar_tfms+tensor_tfm)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset_ar_rs, batch_size=64, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "ar_rs_prec5 = validate(val_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [50/782]\tTime 0.136 (0.565)\tLoss 1.2061 (1.0429)\tPrec@1 65.625 (75.938)\tPrec@5 90.625 (94.031)\n",
      "Test: [100/782]\tTime 0.129 (0.372)\tLoss 1.3998 (1.0477)\tPrec@1 59.375 (76.156)\tPrec@5 87.500 (93.422)\n",
      "Test: [150/782]\tTime 0.126 (0.305)\tLoss 0.7977 (1.0624)\tPrec@1 81.250 (75.594)\tPrec@5 95.312 (92.958)\n",
      "Test: [200/782]\tTime 0.106 (0.305)\tLoss 0.5542 (1.0023)\tPrec@1 85.938 (76.617)\tPrec@5 100.000 (93.547)\n",
      "Test: [250/782]\tTime 0.111 (0.275)\tLoss 0.7883 (0.9978)\tPrec@1 81.250 (76.431)\tPrec@5 93.750 (93.487)\n",
      "Test: [300/782]\tTime 0.134 (0.272)\tLoss 0.9626 (0.9680)\tPrec@1 76.562 (76.969)\tPrec@5 92.188 (93.703)\n",
      "Test: [350/782]\tTime 0.129 (0.262)\tLoss 0.8109 (0.9421)\tPrec@1 85.938 (77.612)\tPrec@5 95.312 (93.902)\n",
      "Test: [400/782]\tTime 0.130 (0.245)\tLoss 0.9166 (0.9237)\tPrec@1 79.688 (77.934)\tPrec@5 93.750 (94.074)\n",
      "Test: [450/782]\tTime 0.129 (0.233)\tLoss 1.1614 (0.9234)\tPrec@1 64.062 (78.066)\tPrec@5 92.188 (94.247)\n",
      "Test: [500/782]\tTime 0.130 (0.222)\tLoss 0.3823 (0.9363)\tPrec@1 90.625 (77.728)\tPrec@5 98.438 (94.059)\n",
      "Test: [550/782]\tTime 0.131 (0.214)\tLoss 1.3419 (0.9474)\tPrec@1 64.062 (77.497)\tPrec@5 95.312 (93.903)\n",
      "Test: [600/782]\tTime 0.129 (0.207)\tLoss 1.2445 (0.9607)\tPrec@1 70.312 (77.211)\tPrec@5 93.750 (93.695)\n",
      "Test: [650/782]\tTime 0.138 (0.206)\tLoss 0.8919 (0.9572)\tPrec@1 79.688 (77.207)\tPrec@5 95.312 (93.764)\n",
      "Test: [700/782]\tTime 0.148 (0.207)\tLoss 0.5315 (0.9544)\tPrec@1 85.938 (77.330)\tPrec@5 95.312 (93.848)\n",
      "Test: [750/782]\tTime 0.150 (0.205)\tLoss 0.6399 (0.9550)\tPrec@1 81.250 (77.352)\tPrec@5 98.438 (93.879)\n",
      "Test: [782/782]\tTime 1.306 (0.237)\tLoss 0.8645 (0.9548)\tPrec@1 75.000 (77.380)\tPrec@5 93.750 (93.916)\n",
      "Total Time:0.05155297777777778\t Top 5 Accuracy: 93.916\n",
      "\n",
      " * Prec@1 77.380 Prec@5 93.916\n"
     ]
    }
   ],
   "source": [
    "val_ar_tfms = [transforms.Resize(int(target_size*1.14)), RectangularCropTfm(idx2ar, target_size)]\n",
    "val_dataset_ar_rs = ValDataset(valdir, val_ar_tfms+tensor_tfm)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset_ar_rs, batch_size=64, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "ar_rs_prec5 = validate(val_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Precision -  AR 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [50/782]\tTime 0.136 (0.181)\tLoss 1.2006 (1.0500)\tPrec@1 67.188 (75.500)\tPrec@5 87.500 (93.844)\n",
      "Test: [100/782]\tTime 0.129 (0.162)\tLoss 1.4244 (1.0571)\tPrec@1 57.812 (75.750)\tPrec@5 87.500 (93.250)\n",
      "Test: [150/782]\tTime 0.126 (0.155)\tLoss 0.8122 (1.0692)\tPrec@1 76.562 (75.229)\tPrec@5 95.312 (92.740)\n",
      "Test: [200/782]\tTime 0.112 (0.146)\tLoss 0.5240 (1.0075)\tPrec@1 87.500 (76.344)\tPrec@5 100.000 (93.305)\n",
      "Test: [250/782]\tTime 0.107 (0.137)\tLoss 0.8130 (1.0045)\tPrec@1 81.250 (76.150)\tPrec@5 95.312 (93.213)\n",
      "Test: [300/782]\tTime 0.121 (0.135)\tLoss 0.9275 (0.9737)\tPrec@1 75.000 (76.620)\tPrec@5 90.625 (93.438)\n",
      "Test: [350/782]\tTime 0.129 (0.135)\tLoss 0.8336 (0.9480)\tPrec@1 84.375 (77.263)\tPrec@5 92.188 (93.634)\n",
      "Test: [400/782]\tTime 0.129 (0.137)\tLoss 0.9164 (0.9290)\tPrec@1 81.250 (77.715)\tPrec@5 95.312 (93.859)\n",
      "Test: [450/782]\tTime 0.129 (0.136)\tLoss 1.1764 (0.9273)\tPrec@1 64.062 (77.976)\tPrec@5 92.188 (94.049)\n",
      "Test: [500/782]\tTime 0.131 (0.136)\tLoss 0.3808 (0.9403)\tPrec@1 93.750 (77.656)\tPrec@5 98.438 (93.881)\n",
      "Test: [550/782]\tTime 0.138 (0.136)\tLoss 1.4056 (0.9528)\tPrec@1 60.938 (77.372)\tPrec@5 92.188 (93.690)\n",
      "Test: [600/782]\tTime 0.140 (0.135)\tLoss 1.2721 (0.9660)\tPrec@1 71.875 (77.135)\tPrec@5 95.312 (93.500)\n",
      "Test: [650/782]\tTime 0.138 (0.135)\tLoss 0.8971 (0.9623)\tPrec@1 79.688 (77.077)\tPrec@5 95.312 (93.594)\n",
      "Test: [700/782]\tTime 0.149 (0.136)\tLoss 0.4985 (0.9599)\tPrec@1 87.500 (77.181)\tPrec@5 95.312 (93.670)\n",
      "Test: [750/782]\tTime 0.150 (0.137)\tLoss 0.6062 (0.9593)\tPrec@1 82.812 (77.246)\tPrec@5 96.875 (93.725)\n",
      "Test: [782/782]\tTime 0.094 (0.138)\tLoss 0.8702 (0.9588)\tPrec@1 75.000 (77.270)\tPrec@5 93.750 (93.776)\n",
      "Total Time:0.02997465722222222\t Top 5 Accuracy: 93.776\n",
      "\n",
      " * Prec@1 77.270 Prec@5 93.776\n"
     ]
    }
   ],
   "source": [
    "val_ar_tfms = [transforms.Resize(int(target_size*1.2)), RectangularCropTfm(idx2ar, target_size)]\n",
    "val_dataset_ar_rs = ValDataset(valdir, val_ar_tfms+tensor_tfm)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset_ar_rs, batch_size=64, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "ar_rs_prec5 = validate(val_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Precision - AR 1.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [50/782]\tTime 0.137 (0.187)\tLoss 1.1954 (1.0575)\tPrec@1 67.188 (75.406)\tPrec@5 89.062 (93.594)\n",
      "Test: [100/782]\tTime 0.130 (0.159)\tLoss 1.3734 (1.0582)\tPrec@1 57.812 (75.516)\tPrec@5 89.062 (93.094)\n",
      "Test: [150/782]\tTime 0.127 (0.150)\tLoss 0.7931 (1.0701)\tPrec@1 78.125 (75.229)\tPrec@5 95.312 (92.771)\n",
      "Test: [200/782]\tTime 0.106 (0.142)\tLoss 0.5465 (1.0070)\tPrec@1 84.375 (76.344)\tPrec@5 100.000 (93.367)\n",
      "Test: [250/782]\tTime 0.110 (0.134)\tLoss 0.8014 (1.0016)\tPrec@1 79.688 (76.194)\tPrec@5 95.312 (93.319)\n",
      "Test: [300/782]\tTime 0.122 (0.132)\tLoss 0.9408 (0.9709)\tPrec@1 76.562 (76.771)\tPrec@5 92.188 (93.542)\n",
      "Test: [350/782]\tTime 0.130 (0.131)\tLoss 0.8067 (0.9451)\tPrec@1 84.375 (77.438)\tPrec@5 95.312 (93.737)\n",
      "Test: [400/782]\tTime 0.129 (0.131)\tLoss 0.9111 (0.9264)\tPrec@1 81.250 (77.777)\tPrec@5 95.312 (93.926)\n",
      "Test: [450/782]\tTime 0.130 (0.131)\tLoss 1.1683 (0.9252)\tPrec@1 62.500 (77.955)\tPrec@5 92.188 (94.122)\n",
      "Test: [500/782]\tTime 0.130 (0.131)\tLoss 0.4018 (0.9386)\tPrec@1 93.750 (77.638)\tPrec@5 98.438 (93.928)\n",
      "Test: [550/782]\tTime 0.131 (0.131)\tLoss 1.3862 (0.9503)\tPrec@1 60.938 (77.435)\tPrec@5 95.312 (93.756)\n",
      "Test: [600/782]\tTime 0.130 (0.132)\tLoss 1.2536 (0.9631)\tPrec@1 70.312 (77.099)\tPrec@5 93.750 (93.565)\n",
      "Test: [650/782]\tTime 0.139 (0.132)\tLoss 0.8915 (0.9597)\tPrec@1 79.688 (77.062)\tPrec@5 95.312 (93.644)\n",
      "Test: [700/782]\tTime 0.148 (0.133)\tLoss 0.5404 (0.9577)\tPrec@1 87.500 (77.125)\tPrec@5 95.312 (93.719)\n",
      "Test: [750/782]\tTime 0.150 (0.134)\tLoss 0.6376 (0.9585)\tPrec@1 79.688 (77.167)\tPrec@5 96.875 (93.767)\n",
      "Test: [782/782]\tTime 0.095 (0.135)\tLoss 0.8365 (0.9579)\tPrec@1 75.000 (77.192)\tPrec@5 93.750 (93.806)\n",
      "Total Time:0.029376980555555557\t Top 5 Accuracy: 93.806\n",
      "\n",
      " * Prec@1 77.192 Prec@5 93.806\n"
     ]
    }
   ],
   "source": [
    "val_ar_tfms = [transforms.Resize(int(target_size*1.16)), RectangularCropTfm(idx2ar, target_size)]\n",
    "val_dataset_ar_rs = ValDataset(valdir, val_ar_tfms+tensor_tfm)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset_ar_rs, batch_size=64, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "ar_rs_prec5 = validate(val_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Precision - AR 1.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [50/782]\tTime 0.148 (0.181)\tLoss 1.2111 (1.0373)\tPrec@1 65.625 (76.188)\tPrec@5 92.188 (93.875)\n",
      "Test: [100/782]\tTime 0.131 (0.157)\tLoss 1.3915 (1.0472)\tPrec@1 56.250 (76.125)\tPrec@5 87.500 (93.391)\n",
      "Test: [150/782]\tTime 0.128 (0.149)\tLoss 0.7584 (1.0594)\tPrec@1 81.250 (75.760)\tPrec@5 96.875 (93.083)\n",
      "Test: [200/782]\tTime 0.112 (0.141)\tLoss 0.5492 (1.0006)\tPrec@1 87.500 (76.711)\tPrec@5 100.000 (93.672)\n",
      "Test: [250/782]\tTime 0.111 (0.134)\tLoss 0.7393 (0.9966)\tPrec@1 82.812 (76.513)\tPrec@5 95.312 (93.519)\n",
      "Test: [300/782]\tTime 0.122 (0.131)\tLoss 0.9863 (0.9679)\tPrec@1 75.000 (76.979)\tPrec@5 90.625 (93.698)\n",
      "Test: [350/782]\tTime 0.131 (0.131)\tLoss 0.8289 (0.9426)\tPrec@1 84.375 (77.589)\tPrec@5 93.750 (93.879)\n",
      "Test: [400/782]\tTime 0.130 (0.131)\tLoss 0.9314 (0.9246)\tPrec@1 81.250 (77.957)\tPrec@5 95.312 (94.070)\n",
      "Test: [450/782]\tTime 0.131 (0.132)\tLoss 1.1557 (0.9245)\tPrec@1 64.062 (78.118)\tPrec@5 92.188 (94.267)\n",
      "Test: [500/782]\tTime 0.142 (0.132)\tLoss 0.3808 (0.9376)\tPrec@1 87.500 (77.772)\tPrec@5 98.438 (94.072)\n",
      "Test: [550/782]\tTime 0.130 (0.132)\tLoss 1.3391 (0.9484)\tPrec@1 57.812 (77.517)\tPrec@5 95.312 (93.898)\n",
      "Test: [600/782]\tTime 0.132 (0.132)\tLoss 1.2171 (0.9612)\tPrec@1 70.312 (77.258)\tPrec@5 95.312 (93.701)\n",
      "Test: [650/782]\tTime 0.149 (0.133)\tLoss 0.8729 (0.9577)\tPrec@1 81.250 (77.245)\tPrec@5 95.312 (93.774)\n",
      "Test: [700/782]\tTime 0.149 (0.134)\tLoss 0.5177 (0.9546)\tPrec@1 85.938 (77.357)\tPrec@5 96.875 (93.833)\n",
      "Test: [750/782]\tTime 0.151 (0.135)\tLoss 0.6375 (0.9549)\tPrec@1 81.250 (77.400)\tPrec@5 96.875 (93.867)\n",
      "Test: [782/782]\tTime 0.096 (0.136)\tLoss 0.8737 (0.9547)\tPrec@1 75.000 (77.404)\tPrec@5 93.750 (93.906)\n",
      "Total Time:0.02951494472222222\t Top 5 Accuracy: 93.906\n",
      "\n",
      " * Prec@1 77.404 Prec@5 93.906\n"
     ]
    }
   ],
   "source": [
    "val_ar_tfms = [transforms.Resize(int(target_size*1.12)), RectangularCropTfm(idx2ar, target_size)]\n",
    "val_dataset_ar_rs = ValDataset(valdir, val_ar_tfms+tensor_tfm)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset_ar_rs, batch_size=64, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "ar_rs_prec5 = validate(val_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [50/782]\tTime 0.136 (0.179)\tLoss 1.2334 (1.0421)\tPrec@1 64.062 (76.031)\tPrec@5 90.625 (93.688)\n",
      "Test: [100/782]\tTime 0.129 (0.155)\tLoss 1.3918 (1.0478)\tPrec@1 59.375 (76.109)\tPrec@5 87.500 (93.344)\n",
      "Test: [150/782]\tTime 0.127 (0.147)\tLoss 0.7676 (1.0617)\tPrec@1 79.688 (75.625)\tPrec@5 96.875 (92.958)\n",
      "Test: [200/782]\tTime 0.106 (0.140)\tLoss 0.5427 (1.0020)\tPrec@1 85.938 (76.523)\tPrec@5 100.000 (93.547)\n",
      "Test: [250/782]\tTime 0.107 (0.133)\tLoss 0.7428 (0.9980)\tPrec@1 81.250 (76.381)\tPrec@5 95.312 (93.431)\n",
      "Test: [300/782]\tTime 0.121 (0.131)\tLoss 0.9664 (0.9687)\tPrec@1 75.000 (76.969)\tPrec@5 92.188 (93.630)\n",
      "Test: [350/782]\tTime 0.129 (0.131)\tLoss 0.8253 (0.9433)\tPrec@1 84.375 (77.607)\tPrec@5 93.750 (93.826)\n",
      "Test: [400/782]\tTime 0.132 (0.131)\tLoss 0.9270 (0.9252)\tPrec@1 81.250 (77.969)\tPrec@5 95.312 (94.012)\n",
      "Test: [450/782]\tTime 0.130 (0.131)\tLoss 1.1620 (0.9246)\tPrec@1 65.625 (78.087)\tPrec@5 92.188 (94.194)\n",
      "Test: [500/782]\tTime 0.130 (0.131)\tLoss 0.3930 (0.9377)\tPrec@1 90.625 (77.725)\tPrec@5 98.438 (94.006)\n",
      "Test: [550/782]\tTime 0.131 (0.131)\tLoss 1.3585 (0.9485)\tPrec@1 59.375 (77.517)\tPrec@5 95.312 (93.795)\n",
      "Test: [600/782]\tTime 0.130 (0.131)\tLoss 1.2321 (0.9616)\tPrec@1 70.312 (77.240)\tPrec@5 93.750 (93.609)\n",
      "Test: [650/782]\tTime 0.140 (0.131)\tLoss 0.8837 (0.9581)\tPrec@1 79.688 (77.233)\tPrec@5 95.312 (93.688)\n",
      "Test: [700/782]\tTime 0.148 (0.132)\tLoss 0.5214 (0.9551)\tPrec@1 85.938 (77.344)\tPrec@5 95.312 (93.786)\n",
      "Test: [750/782]\tTime 0.151 (0.133)\tLoss 0.6488 (0.9557)\tPrec@1 82.812 (77.371)\tPrec@5 96.875 (93.810)\n",
      "Test: [782/782]\tTime 0.095 (0.135)\tLoss 0.8883 (0.9556)\tPrec@1 75.000 (77.402)\tPrec@5 93.750 (93.856)\n",
      "Total Time:0.029245837222222223\t Top 5 Accuracy: 93.856\n",
      "\n",
      " * Prec@1 77.402 Prec@5 93.856\n"
     ]
    }
   ],
   "source": [
    "val_ar_tfms = [transforms.Resize(int(target_size*1.13)), RectangularCropTfm(idx2ar, target_size)]\n",
    "val_dataset_ar_rs = ValDataset(valdir, val_ar_tfms+tensor_tfm)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset_ar_rs, batch_size=64, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "ar_rs_prec5 = validate(val_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [50/782]\tTime 0.140 (0.181)\tLoss 1.2216 (1.0428)\tPrec@1 67.188 (75.281)\tPrec@5 90.625 (93.781)\n",
      "Test: [100/782]\tTime 0.131 (0.157)\tLoss 1.3970 (1.0472)\tPrec@1 57.812 (75.812)\tPrec@5 85.938 (93.219)\n",
      "Test: [150/782]\tTime 0.128 (0.149)\tLoss 0.7578 (1.0622)\tPrec@1 81.250 (75.438)\tPrec@5 95.312 (92.854)\n",
      "Test: [200/782]\tTime 0.108 (0.142)\tLoss 0.5641 (1.0016)\tPrec@1 84.375 (76.484)\tPrec@5 100.000 (93.453)\n",
      "Test: [250/782]\tTime 0.107 (0.135)\tLoss 0.7898 (0.9973)\tPrec@1 79.688 (76.406)\tPrec@5 95.312 (93.406)\n",
      "Test: [300/782]\tTime 0.123 (0.132)\tLoss 0.9487 (0.9669)\tPrec@1 75.000 (76.953)\tPrec@5 92.188 (93.620)\n",
      "Test: [350/782]\tTime 0.132 (0.132)\tLoss 0.7973 (0.9417)\tPrec@1 85.938 (77.585)\tPrec@5 95.312 (93.768)\n",
      "Test: [400/782]\tTime 0.130 (0.132)\tLoss 0.9134 (0.9235)\tPrec@1 81.250 (77.902)\tPrec@5 95.312 (93.977)\n",
      "Test: [450/782]\tTime 0.130 (0.132)\tLoss 1.1607 (0.9233)\tPrec@1 62.500 (78.059)\tPrec@5 92.188 (94.153)\n",
      "Test: [500/782]\tTime 0.131 (0.132)\tLoss 0.3902 (0.9366)\tPrec@1 93.750 (77.709)\tPrec@5 98.438 (93.947)\n",
      "Test: [550/782]\tTime 0.130 (0.132)\tLoss 1.3957 (0.9481)\tPrec@1 62.500 (77.474)\tPrec@5 96.875 (93.778)\n",
      "Test: [600/782]\tTime 0.131 (0.132)\tLoss 1.2389 (0.9611)\tPrec@1 71.875 (77.190)\tPrec@5 93.750 (93.586)\n",
      "Test: [650/782]\tTime 0.146 (0.132)\tLoss 0.9201 (0.9577)\tPrec@1 78.125 (77.137)\tPrec@5 95.312 (93.666)\n",
      "Test: [700/782]\tTime 0.150 (0.133)\tLoss 0.5001 (0.9547)\tPrec@1 87.500 (77.254)\tPrec@5 95.312 (93.743)\n",
      "Test: [750/782]\tTime 0.152 (0.134)\tLoss 0.6257 (0.9546)\tPrec@1 81.250 (77.302)\tPrec@5 98.438 (93.796)\n",
      "Test: [782/782]\tTime 0.095 (0.136)\tLoss 0.8533 (0.9542)\tPrec@1 75.000 (77.334)\tPrec@5 93.750 (93.832)\n",
      "Total Time:0.02943934388888889\t Top 5 Accuracy: 93.832\n",
      "\n",
      " * Prec@1 77.334 Prec@5 93.832\n"
     ]
    }
   ],
   "source": [
    "val_ar_tfms = [transforms.Resize(int(target_size*1.15)), RectangularCropTfm(idx2ar, target_size)]\n",
    "val_dataset_ar_rs = ValDataset(valdir, val_ar_tfms+tensor_tfm)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset_ar_rs, batch_size=64, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "ar_rs_prec5 = validate(val_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [50/782]\tTime 0.138 (0.182)\tLoss 1.2220 (1.0397)\tPrec@1 65.625 (75.750)\tPrec@5 87.500 (93.531)\n",
      "Test: [100/782]\tTime 0.131 (0.157)\tLoss 1.3749 (1.0492)\tPrec@1 57.812 (76.141)\tPrec@5 87.500 (93.297)\n",
      "Test: [150/782]\tTime 0.127 (0.149)\tLoss 0.7334 (1.0605)\tPrec@1 84.375 (75.906)\tPrec@5 96.875 (93.052)\n",
      "Test: [200/782]\tTime 0.107 (0.142)\tLoss 0.5886 (1.0021)\tPrec@1 87.500 (76.750)\tPrec@5 100.000 (93.586)\n",
      "Test: [250/782]\tTime 0.108 (0.134)\tLoss 0.7451 (0.9975)\tPrec@1 84.375 (76.594)\tPrec@5 95.312 (93.438)\n",
      "Test: [300/782]\tTime 0.121 (0.131)\tLoss 0.9844 (0.9688)\tPrec@1 75.000 (77.068)\tPrec@5 90.625 (93.651)\n",
      "Test: [350/782]\tTime 0.139 (0.131)\tLoss 0.8005 (0.9444)\tPrec@1 81.250 (77.701)\tPrec@5 93.750 (93.812)\n",
      "Test: [400/782]\tTime 0.130 (0.131)\tLoss 0.9365 (0.9271)\tPrec@1 79.688 (78.039)\tPrec@5 95.312 (93.980)\n",
      "Test: [450/782]\tTime 0.131 (0.131)\tLoss 1.1455 (0.9272)\tPrec@1 65.625 (78.257)\tPrec@5 92.188 (94.139)\n",
      "Test: [500/782]\tTime 0.131 (0.131)\tLoss 0.3765 (0.9393)\tPrec@1 95.312 (77.912)\tPrec@5 98.438 (93.959)\n",
      "Test: [550/782]\tTime 0.131 (0.131)\tLoss 1.3409 (0.9498)\tPrec@1 57.812 (77.693)\tPrec@5 95.312 (93.804)\n",
      "Test: [600/782]\tTime 0.138 (0.131)\tLoss 1.2156 (0.9626)\tPrec@1 70.312 (77.370)\tPrec@5 92.188 (93.583)\n",
      "Test: [650/782]\tTime 0.140 (0.132)\tLoss 0.8700 (0.9588)\tPrec@1 79.688 (77.361)\tPrec@5 95.312 (93.651)\n",
      "Test: [700/782]\tTime 0.150 (0.133)\tLoss 0.5123 (0.9557)\tPrec@1 84.375 (77.471)\tPrec@5 96.875 (93.728)\n",
      "Test: [750/782]\tTime 0.151 (0.134)\tLoss 0.6311 (0.9556)\tPrec@1 82.812 (77.492)\tPrec@5 96.875 (93.775)\n",
      "Test: [782/782]\tTime 0.095 (0.135)\tLoss 0.8860 (0.9552)\tPrec@1 75.000 (77.524)\tPrec@5 93.750 (93.820)\n",
      "Total Time:0.029371394999999998\t Top 5 Accuracy: 93.820\n",
      "\n",
      " * Prec@1 77.524 Prec@5 93.820\n"
     ]
    }
   ],
   "source": [
    "val_ar_tfms = [transforms.Resize(int(target_size*1.10)), RectangularCropTfm(idx2ar, target_size)]\n",
    "val_dataset_ar_rs = ValDataset(valdir, val_ar_tfms+tensor_tfm)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset_ar_rs, batch_size=64, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "ar_rs_prec5 = validate(val_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_batch_means = [np.array(c).mean() for c in chunks(ar_means, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'DawnValidation': orig_prec5, \n",
    "     'BatchAspectRatioValidation': ar_rs_prec5, \n",
    "     'AR Mean': ar_batch_means[:-1],\n",
    "     'Difference': np.array(ar_rs_prec5)-np.array(orig_prec5)}\n",
    "df = pd.DataFrame(data=d);\n",
    "df.to_csv('ar_tests.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test TTA with original validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_scale = 0.5\n",
    "trn_tfms = [\n",
    "        transforms.RandomResizedCrop(target_size, scale=(min_scale, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ] + tensor_tfm\n",
    "aug_dataset = datasets.ImageFolder(valdir, transforms.Compose(trn_tfms))\n",
    "\n",
    "val_tfms = [transforms.Resize(int(target_size*1.14)), transforms.CenterCrop(target_size)] + tensor_tfm\n",
    "val_dataset = datasets.ImageFolder(valdir,  transforms.Compose(val_tfms))\n",
    "\n",
    "aug_loader = torch.utils.data.DataLoader(\n",
    "    aug_dataset, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True, sampler=val_sampler_ar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_prec5 = validate(val_loader, model, criterion, aug_loader=aug_loader, num_augmentations=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    'AR Mean': ar_batch_means[:-1],\n",
    "    'DawnValidation': orig_prec5, \n",
    "    'TTA': tta_prec5, \n",
    "    'ARValidation': ar_rs_prec5, \n",
    "#     'Difference': np.array(ar_rs_prec5)-np.array(orig_prec5)\n",
    "}\n",
    "df = pd.DataFrame(data=d);\n",
    "df.to_csv('ar_tests_tta.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTA with AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_scale = 0.5\n",
    "trn_tfms = [\n",
    "        transforms.RandomResizedCrop(target_size, scale=(min_scale, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ] + tensor_tfm\n",
    "aug_dataset = datasets.ImageFolder(valdir, transforms.Compose(trn_tfms))\n",
    "\n",
    "aug_loader = torch.utils.data.DataLoader(\n",
    "    aug_dataset, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "val_ar_tfms = [transforms.Resize(int(target_size*1.14)), RectangularCropTfm(idx2ar, target_size)]\n",
    "val_dataset_ar_rs = ValDataset(valdir, val_ar_tfms+tensor_tfm)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset_ar_rs, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True, sampler=val_sampler_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_ar_rs_prec5 = validate(val_loader, model, criterion, aug_loader=aug_loader, num_augmentations=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ar_tests_tta.csv', index_col=0); df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.DataFrame({'TTA_AR': tta_ar_rs_prec5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ar_tests_tta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.join(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1 == pd pd..DataFrameDataFrame({({'dat1''dat1'::  [[99,,55]})]})\n",
    "dat2 == pd pd..DataFrameDataFrame({({'dat2''dat2'::  [[77,,66]})]})\n",
    " >> dat1 dat1..joinjoin((dat2dat2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
